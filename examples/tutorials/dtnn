{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Deep Tensor Neural Networks (DTNNs): A Beginner’s Tutorial**\n",
    "\n",
    "This tutorial will guide you through the key ideas and practical steps behind the _Quantum-chemical insights from deep tensor neural networks_ (DTNN) framework, as presented by Schütt _et al._ (Nature Communications, 2017). By the end, you will understand how DTNNs model molecules, predict energies, and reveal local chemical potentials.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Motivation and Overview\n",
    "\n",
    "- **Why machine learning in quantum chemistry?** Conventional electronic‑structure methods (e.g., Density Functional Theory) are accurate but computationally costly for large chemical spaces. Machine learning offers a data‑driven route to predict molecular properties quickly.\n",
    "\n",
    "- **What are DTNNs?** DTNNs combine quantum‑mechanical many‑body principles with deep learning. They learn an _atom‑centered embedding_ that respects physical invariances (translation, rotation, permutation) and yields energy predictions with chemical accuracy (≈1 kcal/mol).\n",
    "\n",
    "**Core strengths:**\n",
    "- Size‑extensive: scales linearly with the number of atoms\n",
    "- Uniform accuracy across composition and configuration space\n",
    "- Interpretable: yields per‑atom energy contributions and spatially resolved chemical potentials\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Representing a Molecule for DTNN\n",
    "\n",
    "1. **Nuclear Charges Vector**: A list \\(Z = [Z_1, Z_2, \\dots, Z_N]\\) for a molecule of \\(N\\) atoms, encoding element types.\n",
    "2. **Inter‑atomic Distance Matrix**: \\(D\\), an \\(N\\times N\\) matrix where \\(D_{ij} = \\|\\mathbf{r}_i - \\mathbf{r}_j\\|\\).\n",
    "\n",
    "> These inputs guarantee invariance under rotations/translations. DTNN further enforces permutation invariance by summing atomic energies.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Preprocessing Distances: Gaussian Expansion\n",
    "\n",
    "To capture different interaction regimes, distances are expanded into a fixed-size feature vector:\n",
    "\n",
    "\\[\n",
    "\\hat d_{ij,k} = \\exp\\Bigl(-\\frac{(D_{ij} - \\mu_k)^2}{2\\sigma^2}\\Bigr), \\quad k=1,2,\\dots,G\n",
    "\\]\n",
    "\n",
    "- **\\(\\mu_k\\)**: centers on a uniform grid (e.g., from 0 to 20 Å)\n",
    "- **\\(\\sigma\\)**: width (e.g., 0.2 Å)\n",
    "- **Result**: each pair \\((i,j)\\) has a vector \\(\\hat d_{ij} \\in \\mathbb{R}^G\\)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Initial Atomic Embeddings\n",
    "\n",
    "Each atom \\(i\\) is assigned an initial descriptor vector based on its nuclear charge:\n",
    "\n",
    "\\[c_i^{(0)} = \\mathbf{c}_{Z_i} \\in \\mathbb{R}^B\\]\n",
    "\n",
    "- \\(B\\) is the embedding dimension (e.g., 30).\n",
    "- \\(\\mathbf{c}_Z\\) are _trainable_ vectors for each element type.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Interaction Passes: Refining Embeddings\n",
    "\n",
    "DTNN performs \\(T\\) iterative “interaction” layers. At pass \\(t\\), each atom \\(i\\) updates:\n",
    "\n",
    "\\[\n",
    "c_i^{(t+1)} = c_i^{(t)} + \\sum_{j \\neq i} v_{ij}^{(t)}\n",
    "\\]\n",
    "\n",
    "where the pairwise correction uses a _factored tensor layer_:\n",
    "\n",
    "\\[\n",
    "v_{ij} = \\tanh\\bigl( W_f^c(c_j^{(t)}) \\odot W_f^d(\\hat d_{ij}) + b_f \\bigr)\n",
    "\\]\n",
    "\n",
    "- \\(W_f^c: \\mathbb{R}^B\\to\\mathbb{R}^F\\) and \\(W_f^d: \\mathbb{R}^G\\to\\mathbb{R}^F\\)\n",
    "- \\(F\\) is the number of factors (e.g., 60)\n",
    "- ``\\(\\odot\\)`` denotes element‑wise product\n",
    "\n",
    "**Interpretation:** each atom’s embedding is polished by learned interactions with neighbors.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Predicting Atomic and Molecular Energies\n",
    "\n",
    "After \\(T\\) passes, transform each \\(c_i^{(T)}\\) via two feed‑forward layers to yield per‑atom energy contributions \\(\\hat E_i\\). The total energy is:\n",
    "\n",
    "\\[E_{\\rm mol} = \\sum_{i=1}^N \\hat E_i\\]\n",
    "\n",
    "Training minimizes the mean‑squared error against reference DFT energies.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Visualizing Local Chemical Potentials\n",
    "\n",
    "Once trained, DTNN can probe “what if” scenarios by placing a _test atom_ A (e.g., H, C) at various positions around a molecule:\n",
    "\n",
    "1. Treat the probe’s initial \\(c_{\\rm probe}^{(0)}\\) and compute \\(T\\) passes interacting **from** the molecule **to** the probe only.\n",
    "2. The resulting energy of the probe at position \\(r\\) defines a local chemical potential \\(\\Omega_M^A(r)\\).\n",
    "\n",
    "**Use cases:** mapping reactive sites, evaluating ring aromaticity, comparing stability of functional groups.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Practical Tips and Extensions\n",
    "\n",
    "- **Distance cutoff:** for large molecules, ignore pairs beyond a threshold (e.g., 3 Å) to reduce cost.\n",
    "- **Hyperparameters:** common choices: \\(B=30, F=60, G=50, T=2\\). Tune per dataset.\n",
    "- **Data requirements:** ~10⁴–10⁵ DFT calculations for good coverage of chemical space.\n",
    "- **Beyond energies:** can be extended to forces (via gradients), electronic spectra, and alchemical interpolations.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Further Reading\n",
    "\n",
    "- Original DTNN paper: Schütt _et al._, _Nat. Commun._ 8, 13890 (2017).\n",
    "- Related architectures: SchNet (Schütt _et al._, _J. Chem. Phys._ 2018), PhysNet (Unke & Meuwly, 2019).\n",
    "- Tutorials on graph neural networks for molecules: [DeepChem GNN tutorial](https://deepchem.io).\n",
    "\n",
    "*Happy exploring quantum machine learning!*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
