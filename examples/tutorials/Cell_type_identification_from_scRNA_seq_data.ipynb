{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c090f9f",
   "metadata": {},
   "source": [
    "# Introduction to cell type identification from scRNA-seq data using ACTINN\n",
    "\n",
    "Cell type identification is one of the major goals in single cell RNA sequencing (scRNA-seq). Traditional methods for assigning cell types typically involve the use of unsupervised clustering, the identification of signature genes in each cluster, followed by a manual lookup of these genes in the literature and databases to assign cell types. However, there are several limitations associated with these approaches, such as unwanted sources of variation that influence clustering and a lack of canonical markers for certain cell types.[1]\n",
    "\n",
    "In this tutorial, we demonstrate the use of ACTINN (Automated Cell Type Identification using Neural Networks), which employs a neural network with three hidden layers, trains on datasets with predefined cell types and predicts cell types for other datasets based on the trained parameters.\n",
    "\n",
    "Here, ACTINN is implemented with a deepchem wrapper.\n",
    "\n",
    "## Colab\n",
    "\n",
    "This tutorial and the rest in this sequence can be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/Advanced_Model_Training.ipynb)\n",
    "\n",
    "## Setup\n",
    "\n",
    "To run DeepChem within Colab, you'll need to run the following installation commands. You can of course run this tutorial locally if you prefer. In that case, don't run these cells since they will download and install DeepChem in your local machine again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/dc_new/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/Users/harin/Desktop/deepchem/deepchem/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.8.1.dev'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install --pre deepchem\n",
    "import deepchem as dc\n",
    "dc.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.path.dirname(os.path.realpath('__file__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f6418",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.utils.download_url(\n",
    "    'https://github.com/Harindhar10/deepchem/tree/actinn/deepchem/feat/tests/data/sc_rna_seq_data/scRNAseq_sample_1.h5',\n",
    "    current_dir,\n",
    "    'sample_1.h5'\n",
    ")\n",
    "\n",
    "dc.utils.download_url(\n",
    "    'https://github.com/Harindhar10/deepchem/tree/actinn/deepchem/feat/tests/data/sc_rna_seq_data/labels_1.txt',\n",
    "    current_dir,\n",
    "    'labels_1.txt'\n",
    ")\n",
    "\n",
    "dc.utils.download_url(\n",
    "    'https://github.com/Harindhar10/deepchem/tree/actinn/deepchem/feat/tests/data/sc_rna_seq_data/scRNAseq_sample_2.h5',\n",
    "    current_dir,\n",
    "    'sample_2.h5'\n",
    ")\n",
    "\n",
    "dc.utils.download_url(\n",
    "    'https://github.com/Harindhar10/deepchem/tree/actinn/deepchem/feat/tests/data/sc_rna_seq_data/labels_2.txt',\n",
    "    current_dir,\n",
    "    'labels_2.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05127b",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b6cdd",
   "metadata": {},
   "source": [
    "Subsets of data obtained from [Tabula Muris](https://tabula-muris.ds.czbiohub.org) is used for simplicity.\n",
    "\n",
    "There are two approaches to creating the train and test sets:\n",
    "1. Splitting a single dataset into training and testing portions.\n",
    "2. Using datasets from different sources as the train and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f5dc7",
   "metadata": {},
   "source": [
    "### 1. Same source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(os.path.join(current_dir,'labels_1.txt'), header=None, sep='\\t')\n",
    "dataset = pd.read_hdf(os.path.join(current_dir,'sample_1.h5'))\n",
    "\n",
    "# Stratified split based on cell_type\n",
    "train_ids, test_ids = train_test_split(\n",
    "    labels[0],\n",
    "    test_size=0.2,  \n",
    "    stratify=labels[1],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_labels = labels[labels[0].isin(train_ids)]\n",
    "test_labels = labels[labels[0].isin(test_ids)]\n",
    "\n",
    "train_set = dataset.loc[:, train_ids]\n",
    "test_set = dataset.loc[:, test_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ab0af",
   "metadata": {},
   "source": [
    "TODO: Use deepchem splitter instead. Data is stored as genes x cells (features x samples), which isn't compatible with deepchem's splitter and dataset objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a46c3",
   "metadata": {},
   "source": [
    "'labels' has columns cell id and cell type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4969ada",
   "metadata": {},
   "source": [
    "### 2. Different sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb7fce",
   "metadata": {},
   "source": [
    "When using datasets from different sources, the gene sets may not completely overlap. Since the ACTINN model defines its layers based on the gene set, it's essential that both the training and testing sets contain the same genes. To ensure this, we first identified the genes common to both datasets and filtered out those that were not shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea6580",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_hdf(os.path.join(current_dir,'sample_1.h5'))\n",
    "train_labels = pd.read_csv(os.path.join(current_dir,'labels_1.txt'), header=None, sep='\\t')\n",
    "test_set = pd.read_hdf(os.path.join(current_dir,'sample_2.h5'))\n",
    "test_labels = pd.read_csv(os.path.join(current_dir,'labels_2.txt'), header=None, sep='\\t')\n",
    "\n",
    "common_genes = train_set.index.intersection(test_set.index)\n",
    "common_genes = sorted(common_genes)\n",
    "\n",
    "train_set = train_set.loc[common_genes]\n",
    "test_set = test_set.loc[common_genes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaaee07",
   "metadata": {},
   "source": [
    "### To convert the cell type labels from strings to integers. Typically labels are present as 'B Cell', 'T Cell' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_type2label(self, types):\n",
    "\n",
    "    all_celltype = list(set(types))\n",
    "    n_types = len(all_celltype)\n",
    "\n",
    "    type_to_label_dict = {}\n",
    "\n",
    "    for i in range(len(all_celltype)):\n",
    "        type_to_label_dict[all_celltype[i]] = i\n",
    "\n",
    "    types = list(types)\n",
    "    labels = list()\n",
    "    for type in types:\n",
    "        labels.append(type_to_label_dict[type])\n",
    "    return np.array(labels)\n",
    "\n",
    "train_labels = convert_type2label(train_labels[1])\n",
    "test_labels = convert_type2label(test_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f534c",
   "metadata": {},
   "source": [
    "## Train set data transformation and gene filtering\n",
    "\n",
    "Next, each cellâ€™s expression value was normalized to its total expression value and multiplied by a scale factor of 10 000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5661e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) extract gene names & data array\n",
    "gene_names = train_set.index.to_numpy()\n",
    "X = np.array(train_set, dtype=np.float32)\n",
    "\n",
    "# 2) library-size normalize to 10,000 (in-place)\n",
    "col_sums = X.sum(axis=0, keepdims=True)  # shape (1, n_cells)\n",
    "X /= col_sums  # broadcast divide\n",
    "X *= 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9feb0c",
   "metadata": {},
   "source": [
    "The counts were increased by 1, and the log2 value was calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f71611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) log2(x + 1) transform (in-place)\n",
    "np.log2(X + 1, out=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e8870",
   "metadata": {},
   "source": [
    "To filter out outlier genes, the genes with the highest 1% and lowest 1% expression were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d116ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) filter by total expression\n",
    "expr = X.sum(axis=1)  # total per gene\n",
    "low, high = np.percentile(expr, [1, 99])\n",
    "mask_expr = (expr >= low) & (expr <= high)\n",
    "X = X[mask_expr, :]\n",
    "gene_names = gene_names[mask_expr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa231ff",
   "metadata": {},
   "source": [
    "The gene with the highest 1% and the lowest 1% standard deviation were also removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d9f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) filter by coefficient of variation\n",
    "mean_expr = X.mean(axis=1)\n",
    "cv = X.std(axis=1) / mean_expr\n",
    "low_cv, high_cv = np.percentile(cv, [1, 99])\n",
    "mask_cv = (cv >= low_cv) & (cv <= high_cv)\n",
    "X = X[mask_cv, :]\n",
    "gene_names = gene_names[mask_cv]\n",
    "\n",
    "# genes x cells to cells x genes\n",
    "train_set = np.transpose(X)\n",
    "\n",
    "train_genes = gene_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c915c",
   "metadata": {},
   "source": [
    "## Test set transformation and filtering\n",
    "\n",
    "The exact same normalisation aproach used for train set is used for test set.\n",
    "First, each cellâ€™s expression value was normalized to its total expression value and multiplied by a scale factor\n",
    "of 10 000. The counts were increased by 1, and the log2 value was\n",
    "calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04888a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) extract gene names & data array\n",
    "test_genes = test_set.index.to_numpy()\n",
    "X = np.array(test_set, dtype=np.float32)\n",
    "\n",
    "# 2) library-size normalize to 10,000 (in-place)\n",
    "col_sums = X.sum(axis=0, keepdims=True)  # shape (1, n_cells)\n",
    "X /= col_sums  # broadcast divide\n",
    "X *= 10000\n",
    "\n",
    "# 3) log2(x + 1) transform (in-place)\n",
    "np.log2(X + 1, out=X)\n",
    "\n",
    "# genes x cells --> cells x genes\n",
    "test_set = np.transpose(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16877b4",
   "metadata": {},
   "source": [
    "\n",
    "The gene list obtained from the train set filtering steps is used to mask the test set. In the original ACTINN implementation, genes were filtered using both the training and test sets. To avoid potential information leakage, we instead apply filtering based solely on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e47e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.loc[:, train_genes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683a7a7",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74064094",
   "metadata": {},
   "source": [
    "ACTINN implemented with deepchem Model class as wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a27609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from deepchem.models.torch_models import TorchModel\n",
    "from deepchem.models.optimizers import Adam\n",
    "from deepchem.models.optimizers import ExponentialDecay\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ActinnClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim=None, input_size=None):\n",
    "        \"\"\"\n",
    "        The Classifer class: We are developing a model similar to ACTINN for good accuracy\n",
    "        \"\"\"\n",
    "        if output_dim == None or input_size == None:\n",
    "            raise ValueError('Must explicitly declare input dim (num features) and output dim (number of classes)')\n",
    "\n",
    "        super(ActinnClassifier, self).__init__()\n",
    "        self.inp_dim = input_size\n",
    "        self.out_dim = output_dim\n",
    "\n",
    "        # feed forward layers\n",
    "        self.classifier_sequential = nn.Sequential(\n",
    "                                        nn.Linear(self.inp_dim, 100),\n",
    "                                        nn.ReLU(),\n",
    "\n",
    "                                        nn.Linear(100, 50),\n",
    "                                        nn.ReLU(),\n",
    "\n",
    "                                        nn.Linear(50, 25),\n",
    "                                        nn.ReLU(),\n",
    "\n",
    "                                        nn.Linear(25, output_dim)\n",
    "                                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the classifier\n",
    "        \"\"\"\n",
    "        out = self.classifier_sequential(x)\n",
    "        return out\n",
    "\n",
    "class ACTINNModel(TorchModel):\n",
    "    def __init__(self, output_dim = None, input_size = None, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.model = ActinnClassifier(output_dim, input_size)\n",
    "        \n",
    "        print('model', self.model)\n",
    "        cf_optimizer = Adam(learning_rate=0.0001,\n",
    "                            beta1=0.9,\n",
    "                            beta2=0.999,\n",
    "                            epsilon=1e-08,\n",
    "                            weight_decay=0.005,\n",
    "                            )\n",
    "\n",
    "        cf_decayRate = 0.95\n",
    "        cf_lr_scheduler = ExponentialDecay(initial_rate=0.0001, decay_rate=cf_decayRate, decay_steps=1000)\n",
    "        super(ACTINNModel,\n",
    "              self).__init__(self.model,\n",
    "                             loss=self.loss_fn,\n",
    "                             optimizer=cf_optimizer,\n",
    "                             learning_rate=cf_lr_scheduler,\n",
    "                             output_types=['prediction'],\n",
    "                             **kwargs)\n",
    "\n",
    "    def loss_fn(self, outputs: List, labels: List[torch.Tensor],\n",
    "                    weights: List[torch.Tensor]) -> torch.Tensor:\n",
    "        outputs = outputs[0]\n",
    "        labels = labels[0][:,0]\n",
    "        return nn.CrossEntropyLoss()(outputs,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d3977",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f37ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ACTINNModel(output_dim= n_types,input_size= train_set.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae147813",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, nb_epoch=5)\n",
    "logits= model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcc69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcf4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_tensor = torch.from_numpy(logits)\n",
    "probabilities = F.softmax(logits_tensor, dim=1)  # Shape: (100, 12)\n",
    "predictions = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ff305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf587574",
   "metadata": {},
   "source": [
    "### References\n",
    "1. [ACTINN: automated identification of cell types in single cell RNA sequencing](https://academic.oup.com/bioinformatics/article/36/2/533/5540320)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
