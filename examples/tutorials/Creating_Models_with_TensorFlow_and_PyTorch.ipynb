{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyDvil_3p0NA"
      },
      "source": [
        "#  Creating Models with TensorFlow and PyTorch\n",
        "\n",
        "In the tutorials so far, we have used standard models provided by DeepChem.  This is fine for many applications, but sooner or later you will want to create an entirely new model with an architecture you define yourself.  DeepChem provides integration with both TensorFlow (Keras) and PyTorch, so you can use it with models from either of these frameworks.\n",
        "\n",
        "## Colab\n",
        "\n",
        "This tutorial and the rest in this sequence are designed to be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/Creating_Models_with_TensorFlow_and_PyTorch.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6KbbaYENp0NE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b3ff57-c079-4743-8c5e-927c141672de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.2.dev20230428125349-py3-none-any.whl (775 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.8/775.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit\n",
            "  Downloading rdkit-2023.3.1b1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.10.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (8.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: rdkit, deepchem\n",
            "Successfully installed deepchem-2.7.2.dev20230428125349 rdkit-2023.3.1b1\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre deepchem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUG4mG9rp0NH"
      },
      "source": [
        "There are actually two different approaches you can take to using TensorFlow or PyTorch models with DeepChem.  It depends on whether you want to use TensorFlow/PyTorch APIs or DeepChem APIs for training and evaluating your model.  For the former case, DeepChem's `Dataset` class has methods for easily adapting it to use with other frameworks.  `make_tf_dataset()` returns a `tensorflow.data.Dataset` object that iterates over the data.  `make_pytorch_dataset()` returns a `torch.utils.data.IterableDataset` that iterates over the data.  This lets you use DeepChem's datasets, loaders, featurizers, transformers, splitters, etc. and easily integrate them into your existing TensorFlow or PyTorch code.\n",
        "\n",
        "But DeepChem also provides many other useful features.  The other approach, which lets you use those features, is to wrap your model in a DeepChem `Model` object.  Let's look at how to do that.\n",
        "\n",
        "## KerasModel\n",
        "\n",
        "`KerasModel` is a subclass of DeepChem's `Model` class.  It acts as a wrapper around a `tensorflow.keras.Model`.  Let's see an example of using it.  For this example, we create a simple sequential model consisting of two dense layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N4slcJjQp0NI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af646c3-c3ff-4a8f-8e80-6ae3b53ac1c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for AvgIpc. Feature removed!\n",
            "WARNING:deepchem.models.torch_models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/usr/local/lib/python3.10/dist-packages/deepchem/models/torch_models/__init__.py)\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        }
      ],
      "source": [
        "import deepchem as dc\n",
        "import tensorflow as tf\n",
        "\n",
        "keras_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1000, activation='relu'),\n",
        "    tf.keras.layers.Dropout(rate=0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model = dc.models.KerasModel(keras_model, dc.models.losses.L2Loss())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HJiUFj_p0NK"
      },
      "source": [
        "For this example, we used the Keras Sequential class. Our model consists of a dense layer with ReLU activation, 50% dropout to provide regularization, and a final layer that produces a scalar output. We also need to specify the loss function to use when training the model, in this case L2 loss. We can now train and evaluate the model exactly as we would with any other DeepChem model. For example, let's load the Delaney solubility dataset. How does our model do at predicting the solubilities of molecules based on their extended-connectivity fingerprints (ECFPs)?\n",
        "\n",
        "We are using molnet module to load the Delaney solubility dataset with an ECFP featurizer and a random splitter.The dataset is split into training, validation, and test sets, and a deep learning model is trained on the training set for 50 epochs.\n",
        "\n",
        "After training, the model's performance is evaluated on both the training and test sets using the Pearson correlation coefficient as the evaluation metric. The dc.metrics.Metric class is used to specify the metric, and the evaluate method of the model object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlCT4pk4p0NL",
        "outputId": "ac8f5827-f140-4494-dcbf-c54a39a59436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set score: {'pearson_r2_score': 0.976587339319848}\n",
            "test set score: {'pearson_r2_score': 0.7120995515340186}\n"
          ]
        }
      ],
      "source": [
        "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='ECFP', splitter='random')\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "model.fit(train_dataset, nb_epoch=50)\n",
        "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
        "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('test set score:', model.evaluate(test_dataset, [metric]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0-wyR0Rp0NM"
      },
      "source": [
        "## TorchModel\n",
        "\n",
        "`TorchModel` works just like `KerasModel`, except it wraps a `torch.nn.Module`.  Let's use PyTorch to create another model just like the previous one and train it on the same data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14aM8LLwp0NN",
        "outputId": "7de82883-e252-4d34-bba0-7723565e92ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set score: {'pearson_r2_score': 0.9772530149721342}\n",
            "test set score: {'pearson_r2_score': 0.6966383789365864}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "pytorch_model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(1024, 1000),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(0.5),\n",
        "    torch.nn.Linear(1000, 1)\n",
        ")\n",
        "model = dc.models.TorchModel(pytorch_model, dc.models.losses.L2Loss())\n",
        "\n",
        "model.fit(train_dataset, nb_epoch=50)\n",
        "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('test set score:', model.evaluate(test_dataset, [metric]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIuncKuIp0NP"
      },
      "source": [
        "## Computing Losses\n",
        "\n",
        "Now let's see a more advanced example.  In the above models, the loss was computed directly from the model's output.  Often that is fine, but not always.  Consider a classification model that outputs a probability distribution.  While it is possible to compute the loss from the probabilities, it is more numerically stable to compute it from the logits.\n",
        "\n",
        "We are defining a ClassificationModel class, which inherits from the tf.keras.Model class. \n",
        "The __init__ method of the ClassificationModel class defines the model architecture by instantiating the two dense layers using the tf.keras.layers.Dense class. The first dense layer has 1000 units and uses the ReLU activation function, while the second dense layer has a single unit and no activation function.\n",
        "\n",
        "The call method of the ClassificationModel class defines how input data is processed by the model. The input data is passed through the first dense layer, and dropout with a rate of 0.5 is applied during training. The output of the second dense layer is the logits, which are used to compute the cross-entropy loss. The output of the model is the sigmoid of the logits, which is a probability between 0 and 1.\n",
        "\n",
        "The ClassificationModel class is then used to instantiate a keras_model object, which is passed to the dc.models.KerasModel class to create the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-2g9yxs3p0NR"
      },
      "outputs": [],
      "source": [
        "class ClassificationModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(1000, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        y = self.dense1(inputs)\n",
        "        if training:\n",
        "            y = tf.nn.dropout(y, 0.5)\n",
        "        logits = self.dense2(y)\n",
        "        output = tf.nn.sigmoid(logits)\n",
        "        return output, logits\n",
        "\n",
        "keras_model = ClassificationModel()\n",
        "output_types = ['prediction', 'loss']\n",
        "model = dc.models.KerasModel(keras_model, dc.models.losses.SigmoidCrossEntropy(), output_types=output_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SD72U4Zp0NS"
      },
      "source": [
        "We can train our model on the BACE dataset.  This is a binary classification task that tries to predict whether a molecule will inhibit the enzyme BACE-1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwxiVh4cp0NT",
        "outputId": "b3fac3de-ee54-45e5-b269-55cf65950752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set score: {'roc_auc_score': 0.9996312076552352}\n",
            "test set score: {'roc_auc_score': 0.7760869565217392}\n"
          ]
        }
      ],
      "source": [
        "tasks, datasets, transformers = dc.molnet.load_bace_classification(feturizer='ECFP', splitter='scaffold')\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "model.fit(train_dataset, nb_epoch=100)\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('test set score:', model.evaluate(test_dataset, [metric]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf4tIfaPs1Iw"
      },
      "source": [
        "Similarly, we will create a custom Classifier Model class to be used with `TorchModel`. Using similar reasoning to the above `KerasModel`, a custom model allows for easy capturing of the unscaled output (logits in Tensorflow) of the second dense layer. The custom class allows definition of how forward pass is done; enabling capture of the logits right before the final sigmoid is applied to produce the prediction. \n",
        "\n",
        "Finally, an instance of `ClassificationModel` is coupled with a loss function that requires both the prediction and logits to produce an instance of `TorchModel` to train. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jMOOa20Yszz2"
      },
      "outputs": [],
      "source": [
        "class ClassificationModel(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.dense1 = torch.nn.Linear(1024, 1000)\n",
        "        self.dense2 = torch.nn.Linear(1000, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        y = torch.nn.functional.relu( self.dense1(inputs) )\n",
        "        y = torch.nn.functional.dropout(y, p=0.5, training=self.training)\n",
        "        logits = self.dense2(y)\n",
        "        output = torch.sigmoid(logits)\n",
        "        return output, logits\n",
        "\n",
        "torch_model = ClassificationModel()\n",
        "output_types = ['prediction', 'loss']\n",
        "model = dc.models.TorchModel(torch_model, dc.models.losses.SigmoidCrossEntropy(), output_types=output_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQqp2m6iGdgv"
      },
      "source": [
        "We will use the same BACE dataset. As before, the model will try to do a binary classification task that tries to predict whether a molecule will inhibit the enzyme BACE-1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N49-Xsic-9iY",
        "outputId": "01f5d45f-c55b-47ab-ae5d-2ae6f626b2af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set score: {'roc_auc_score': 0.9996032688412377}\n",
            "test set score: {'roc_auc_score': 0.771965579710145}\n"
          ]
        }
      ],
      "source": [
        "tasks, datasets, transformers = dc.molnet.load_bace_classification(feturizer='ECFP', splitter='scaffold')\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "model.fit(train_dataset, nb_epoch=100)\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
        "print('test set score:', model.evaluate(test_dataset, [metric]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LY7QO9ip0NU"
      },
      "source": [
        "## Other Features\n",
        "\n",
        "`KerasModel` and `TorchModel` have lots of other features.  Here are some of the more important ones.\n",
        "\n",
        "- Automatically saving checkpoints during training.\n",
        "- Logging progress to the console, to [TensorBoard](https://www.tensorflow.org/tensorboard), or to [Weights & Biases](https://docs.wandb.com/).\n",
        "- Custom loss functions that you define with a function of the form `f(outputs, labels, weights)`.\n",
        "- Early stopping using the `ValidationCallback` class.\n",
        "- Loading parameters from pre-trained models.\n",
        "- Estimating uncertainty in model outputs.\n",
        "- Identifying important features through saliency mapping.\n",
        "\n",
        "By wrapping your own models in a `KerasModel` or `TorchModel`, you get immediate access to all these features.  See the API documentation for full details on them.\n",
        "\n",
        "# Congratulations! Time to join the Community!\n",
        "\n",
        "Congratulations on completing this tutorial notebook! If you enjoyed working through the tutorial, and want to continue working with DeepChem, we encourage you to finish the rest of the tutorials in this series. You can also help the DeepChem community in the following ways:\n",
        "\n",
        "## Star DeepChem on [GitHub](https://github.com/deepchem/deepchem)\n",
        "This helps build awareness of the DeepChem project and the tools for open source drug discovery that we're trying to build.\n",
        "\n",
        "## Join the DeepChem Gitter\n",
        "The DeepChem [Gitter](https://gitter.im/deepchem/Lobby) hosts a number of scientists, developers, and enthusiasts interested in deep learning for the life sciences. Join the conversation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOBd6-YdQSvF"
      },
      "source": [
        "## Citing This Tutorial\n",
        "If you found this tutorial useful please consider citing it using the provided BibTeX. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZUk_9yIYw0c"
      },
      "outputs": [],
      "source": [
        "@manual{Intro1, \n",
        " title={5}, \n",
        " organization={DeepChem},\n",
        " author={Ramsundar, Bharath and Rebel, Alles}, \n",
        " howpublished = {\\url{https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Creating_Models_with_TensorFlow_and_PyTorch.ipynb}}, \n",
        " year={2021}, \n",
        "} "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}