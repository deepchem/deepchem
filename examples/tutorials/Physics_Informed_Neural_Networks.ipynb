{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Writing Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VIGNESHinZONE/Beginners-level-ML-projects/blob/master/Writing_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtfeQrPnBpLe"
      },
      "source": [
        "# Tutorial 33: Physics Informed Neural Networks using JaxModel & PINN_Model\n",
        " - [Vignesh Venkataraman](https://github.com/VIGNESHinZONE)\n",
        "\n",
        "\n",
        "## Contents\n",
        "\n",
        "- Physics Informed Neural Networks\n",
        "- Setup\n",
        "- Brief about Jax and Autodiff\n",
        "- Burger's Equation\n",
        "- Data Visualisation\n",
        "- Explanation of the Solution using Jax\n",
        "- Usage of PINN Model\n",
        "- Visualize the final results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQcs3Oz1m1ud"
      },
      "source": [
        "#Physics Informed Neural Networks\n",
        "\n",
        "PINNs was introduced by Maziar Raissi et. al in their paper [Physics Informed Deep Learning (Part I): Data-driven\n",
        "Solutions of Nonlinear Partial Differential Equations](https://arxiv.org/abs/1711.10561) which are used for solving supervised learning tasks and also follow an underlying differential equation derived from understanding the Physics. In more simple terms, we try solving a differential equation with a neural network and using the differential equation as the regulariser in the loss function.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "### Here is an illustration of PINNs using a simple differential equation-\n",
        "\n",
        "\n",
        "$\\quad \\quad \\quad \\frac{df}{dt} = f(u, t), \\quad  $ where initial condition is $\\ \\ u(t=0) = u_0$\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "We approximate function $u(t)$ using a Neural Network as $NN(t)$ and apply the following loss function - \n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "  Initial Loss: $\\quad  L_0 = (NN(t=0) - u_0)^2$\n",
        "\n",
        "  Regulariser Loss:= $\\quad L_r = | \\frac{d NN (t) }{d t} - f(NN(t), t) |$\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "And we minimise the $Total Loss$ using Backpropagation-\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "  Total Loss = Initial Loss + Regulariser Loss\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### Here is a technical definition of PINNs taken from the author's official blog-\n",
        "\n",
        "  $\\quad \\quad u_t + \\mathcal{N}[u] = 0,\\ x \\in \\Omega, \\ t\\in[0,T],$\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "where $\\ u(t,x) \\ $ denotes the latent (hidden) solution, $\\ N[⋅] \\ $ is a nonlinear differential operator, and $Ω$ is a subset of $\\ \\mathbb{R}^D \\ $ , and proceed by approximating $u(t,x)$ by a deep neural network. We define $\\ f(t,x) \\ $ to be given by\n",
        "\n",
        "<br>\n",
        " \n",
        "  $\\quad \\quad f := u_t + \\mathcal{N}[u],$\n",
        "\n",
        "<br>\n",
        "\n",
        "This assumption results in a physics informed neural network $ \\ f(t,x) \\ $. This network can be derived by the calculus on computational graphs: Backpropagation.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "### Useful Resources to learn more about PINNs\n",
        "\n",
        "\n",
        "- [Maziar Raissi et. al. official blog on PINNs](https://maziarraissi.github.io/PINNs/)\n",
        "- [Chris Rackauckas's lecture on PINNs lecture 3](https://github.com/mitmath/18337#lecture-3-introduction-to-scientific-machine-learning-through-physics-informed-neural-networks): these lectures are in Julia programming language but still are a great source of learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoZLoq3mB1HJ"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k74sLLulvo1P",
        "outputId": "69f9da73-5d23-4d23-b4dd-b9768fbb8d4f"
      },
      "source": [
        "### In case if you are running this notebook in Local Machine and have CUDA installed then uncomment this below line. ###\n",
        "### Change the cuda version depending on your setup. Here Cuda 11.x is represented as `cuda111`\n",
        "# !pip install --upgrade pip\n",
        "# !pip install --upgrade \"jax[cuda111]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "!pip install deepchem[jax]\n",
        "!pip install pyDOE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepchem[jax]\n",
            "  Downloading deepchem-2.5.0-py3-none-any.whl (552 kB)\n",
            "\u001b[K     |████████████████████████████████| 552 kB 5.4 MB/s \n",
            "\u001b[33mWARNING: deepchem 2.5.0 does not provide the extra 'jax'\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from deepchem[jax]) (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from deepchem[jax]) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deepchem[jax]) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepchem[jax]) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deepchem[jax]) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->deepchem[jax]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->deepchem[jax]) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem[jax]) (1.15.0)\n",
            "Installing collected packages: deepchem\n",
            "Successfully installed deepchem-2.5.0\n",
            "Collecting pyDOE\n",
            "  Downloading pyDOE-0.3.8.zip (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.4.1)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18184 sha256=4da795f38a3c13631115b175ca51b0cca2ebc9226d122d2f90ddf4975609baaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/ce/8a/87b25c685bfeca1872d13b8dc101e087a9c6e3fb5ebb47022a\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrP1qEA5odct"
      },
      "source": [
        "import deepchem as dc\n",
        "import os\n",
        "\n",
        "PINNs_URL = \"https://deepchemdata.s3.us-west-1.amazonaws.com/datasets/raissi_pinns_data.tar.gz\"\n",
        "dc.utils.data_utils.download_url(\n",
        "    url=PINNs_URL, dest_dir=os.getcwd())\n",
        "targz_file = os.path.join(os.getcwd(), 'raissi_pinns_data.tar.gz')\n",
        "dc.utils.data_utils.untargz_file(targz_file, os.getcwd())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781N6-jNCobR"
      },
      "source": [
        "# Brief about Jax and Autodiff\n",
        "\n",
        "- Mention the usage of jax and its functional style\n",
        "- Mention about the Autodiff cookbook from Jax\n",
        "- Asking them to take a look at Haiku and optax library\n",
        "- About the JaxModel and PINN_Model superclass\n",
        "\n",
        "Deepchem has recently introduced Jax support for building models and `JaxModel` superclass is the main API for building "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9aeJa8HC_e9"
      },
      "source": [
        "# Burgers Equation\n",
        "\n",
        "Burgers’ equation is a partial differential equation that was originally proposed as a simplified model of turbulence as exhibited by the full-fledged Navier-Stokes equations. It is a nonlinear equation for which exact solutions are known and is therefore important as a benchmark problem for numerical methods. [More Refrence](https://www.azimuthproject.org/azimuth/show/Burgers%27+equation)\n",
        "\n",
        "<br>\n",
        "\n",
        "Here is the differential Equation we are trying to solve\n",
        "\n",
        "  $\\begin{array}{l}\n",
        "  \\ \\ \\ u_t + u u_x - (0.01/\\pi) u_{xx} = 0,\\ \\ \\ x \\in [-1,1],\\ \\ \\ t \\in [0,1]\n",
        "  \\end{array}$\n",
        "\n",
        "<br>\n",
        "\n",
        "Here are the initial conditions\n",
        "\n",
        "  $\\ \\ \\ u(x, 0) = -\\sin(\\pi x),$\n",
        "\n",
        "  $\\ \\ \\ u(-1, t) = u(1, t) = 0.0$\n",
        "\n",
        "<br>\n",
        "\n",
        "Now let us define:\n",
        "\n",
        "  $\n",
        "  \\ \\ \\ f := u_t + u u_x - (0.01/\\pi) u_{xx},\n",
        "  $\n",
        "\n",
        "and we approximate $u(x, t)$ using Neural Network as $NN(\\theta, x, t)$ where $\\theta$ are the weights of neural networks \n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "</br>\n",
        "Now here are the three main data points that will be used for training our Neural Network to approximate $u(x,t)$\n",
        "\n",
        "\n",
        "\n",
        "1.   We will train points lying between $x \\in [-1,1]$ and $t=0$ to follow as part of the L2 Loss\n",
        "\n",
        "    $min\\ \\ _\\theta \\ \\ (NN(\\theta, x, t) + \\sin(\\pi x))^2$\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "2.   We will train points lying between $t \\in [0,1]$ and $x= \\pm1 $ as part of the L2 Loss\n",
        "\n",
        "    $min\\ \\ _\\theta \\ \\ (NN(\\theta, x, t) + \\sin(\\pi x))^2$\n",
        "\n",
        "<br>\n",
        "\n",
        "3.   We will train points lying between $x \\in [-1,1],\\ \\ \\ t \\in [0,1]$ as part of the regulariser loss\n",
        "\n",
        "    $f(\\theta, x, t):= \\ \\ \\frac{\\partial NN(\\theta, x, t)}{\\partial t} + NN(\\theta, x, t)\\frac{\\partial NN(\\theta, x, t)}{\\partial x} - (0.01/\\pi)\\frac{\\partial^2 NN(\\theta, x, t)}{\\partial^2 x} $\n",
        "\n",
        "    $min\\ \\ _\\theta \\ \\ f(\\theta, x, t)$\n",
        "\n",
        "</br>\n",
        "\n",
        "In this tutorial, we will be combing data conditions 1 and 2 under the same L2Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5naU_8Q7DMVE"
      },
      "source": [
        "# Data Visualisation of the Burgers Equation\n",
        "\n",
        "Now lets load the Burger's Data provided from the author. `pre_process_shock_data ` is used to load the data in a format suitable for Neural Networks. Understanding this function is not neccesary for working through the tutorials.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPCYI5A302F5"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "# Create Dataset\n",
        "\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "import numpy as np\n",
        "import random\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "def pre_process_shock_data(data):\n",
        "\n",
        "  N_u = 100\n",
        "  N_f = 10000\n",
        "\n",
        "  t = data['t'].flatten()[:,None]\n",
        "  x = data['x'].flatten()[:,None]\n",
        "  Exact = np.real(data['usol']).T\n",
        "\n",
        "  X, T = np.meshgrid(x,t)\n",
        "\n",
        "  X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "  u_star = Exact.flatten()[:,None]              \n",
        "\n",
        "  # Doman bounds\n",
        "  lb = X_star.min(0)\n",
        "  ub = X_star.max(0)    \n",
        "\n",
        "  xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "  uu1 = Exact[0:1,:].T\n",
        "  xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "  uu2 = Exact[:,0:1]\n",
        "  xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "  uu3 = Exact[:,-1:]\n",
        "\n",
        "  X_u_train = np.vstack([xx1, xx2, xx3])\n",
        "  X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "  X_f_train = np.vstack((X_f_train, X_u_train))\n",
        "  u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "  idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "  X_u_train = X_u_train[idx, :]\n",
        "  u_train = u_train[idx,:]\n",
        "  return X_u_train, u_train, X_f_train, X_star\n",
        "\n",
        "mat_data = scipy.io.loadmat(os.path.join(os.getcwd(), 'PINNs/burgers_shock.mat'))\n",
        "labeled_X, labeled_y, unlabeled_X, full_domain = pre_process_shock_data(mat_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVJL8882R3_-"
      },
      "source": [
        "We have three Numpy arrays `labeled_X`, `labeled_y` and `unlabeled_X` which will be used for training our neural network,\n",
        "\n",
        "1) `labeled_X` consists of  $x \\in [-1,1]$ & $t=0$ and $t \\in [0,1]$ & $x= \\pm1 $. `labeled_y` has the value of $u(x, t)$:\n",
        "\n",
        "Let us verify that `labeled_X` & `labeled_y` also consists of data points satisfying the condition of \n",
        "\n",
        "  $\\ \\ \\ u(x, 0) = -\\sin(\\pi x), \\quad \\quad x \\in [-1,1]$ & $t=0$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "gm6Grl0M9xee",
        "outputId": "31b8aac4-da13-4cfe-e764-1c77c046811b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "ind = labeled_X[:, 1] == 0.0\n",
        "print(f\"Number of Datapoints with with t = 0 is {len(labeled_X[labeled_X[:, 1] == 0.0])}\")\n",
        "plt.scatter(labeled_X[ind][:, 0], labeled_y[ind], color = 'red', marker = \"o\", alpha = 0.3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f721a00b390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSUzLfi5VUUA"
      },
      "source": [
        "\n",
        "\n",
        "Let us verify that at `labeled_X` & `labeled_y` also consists of datapoints satisfying the condition of \n",
        "\n",
        "  $\\ \\ \\ u(-1, t) = u(1, t) = 0.0, \\quad \\quad t \\in [0,1]$ & $x= \\pm1$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnbzgRQb-3NC",
        "outputId": "6cdf59b8-6a31-494b-ee7a-c0ab3208c7f4"
      },
      "source": [
        "ind = np.abs(labeled_X[:, 0]) == 1.0\n",
        "print(f\"Number of Datapoints with with |x| = 1 is {len(labeled_X[np.abs(labeled_X[:, 0]) == 1.0])}\")\n",
        "np.max(labeled_y[ind]), np.min(labeled_y[ind]), np.mean(labeled_y[ind])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.326502818752141e-16, -1.0854441803149965e-16, 8.236830416577723e-17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndR7ndNLDbiz"
      },
      "source": [
        "# Explanation of the solution\n",
        "\n",
        "We will be using Deepchem's `PINNModel` class to solve Burger's Equation which is based out of `Jax` library. We will approximate $u(x, t)$ using a Neural Network represented as $NN(\\theta, x, t)$\n",
        "\n",
        "For our purpose, we will be using the Haiku library for building neural networks. Due to the functional nature of Jax, we define neural network with two things\n",
        "\n",
        "- Parameters - which act as the weight matrices, upon which Backpropagation is applied for optimisation.\n",
        "- forward_fn - This defines how the weights are used for computing the outputs. Ex- Feedforward, Convolution, etc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht0fLWSQkMW2"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "\n",
        "def f(x, y, z, t):\n",
        "  x = jnp.hstack([x, t])\n",
        "  net = hk.nets.MLP(output_sizes = [20, 20, 20, 20, 20, 20, 20, 20, 1],\n",
        "                    activation = jnp.tanh)\n",
        "  return net(x)\n",
        "\n",
        "init_params, forward_fn = hk.transform(f)\n",
        "rng = jax.random.PRNGKey(500)\n",
        "x_init, t_init = jnp.split(X_u_train, 2, 1)\n",
        "params = init_params(rng, x_init, t_init)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fTsxSnEkYoe"
      },
      "source": [
        "As per the docstrings of PINNModel, we require two additional functions in the given format - \n",
        "\n",
        "1. Create a gradient_fn which tells us about how to compute the gradients of the function-\n",
        "```\n",
        "  >>>\n",
        "  >> def gradient_fn(forward_fn, loss_outputs, initial_data):\n",
        "  >>  def model_loss(params, target, weights, rng, ...):\n",
        "  >>    # write code using the arguments.\n",
        "  >>    # ... indicates the variable number of positional arguments.\n",
        "  >>    return\n",
        "  >>  return model_loss\n",
        "```\n",
        "\n",
        "And to understand more about PINNModel, you can see that the same gradient_fn gets called in the code for computing the gradients.\n",
        "\n",
        "For our purpose, we have two variables $(x, t)$ and we need to tell the PINN Model how to compute the final gradient. For carrying out this process we will be using these main features from jax library for calculating the loss - \n",
        "\n",
        "1. [vmap](https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html) - This for parallelising computations in batches. We will process each row of the dataset, but it will get batched automatically using this feature. \n",
        "2. [jacrev](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobians-and-hessians-using-jacfwd-and-jacrev) - This is used to calculate the jacobian matrix. In our case, the output is a single dimension and hence it can be thought of as the gradient function. We could directly use jax's [grad](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html?highlight=grad#jax-first-transformation-grad) function but using `jacrev` simplifies the array shapes and hence is easier.\n",
        "\n",
        "\n",
        "We need to compute two losses for solving our differential equation-\n",
        "\n",
        "1. Initial Loss\n",
        "\n",
        "```\n",
        "u_pred = forward_fn(params, rng, x_b, t_b)\n",
        "initial_loss = jnp.mean((u_pred - boundary_target) ** 2)\n",
        "```\n",
        "\n",
        "2. Regulariser Loss\n",
        "\n",
        "This is slightly complicated as we need to compute \n",
        "\n",
        "$f(\\theta, x, t):= \\ \\ \\frac{\\partial NN(\\theta, x, t)}{\\partial t} + NN(\\theta, x, t)\\frac{\\partial NN(\\theta, x, t)}{\\partial x} - (0.01/\\pi)\\frac{\\partial^2 NN(\\theta, x, t)}{\\partial^2 x} $\n",
        "\n",
        "The partial derivative operation in the first and second terms can be calculated using `jacrev` function-\n",
        "```\n",
        "u_x, u_t = jacrev(forward_fn, argnums=(2, 3))(params, rng, x, t)\n",
        "```\n",
        "\n",
        "The second partial derivative operation in the third term can be applying `jacrev` twice-\n",
        "\n",
        "```\n",
        "u_xx = jacrev(jacrev(forward_fn, argnums=2), argnums=2)(params, rng, x, t)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwTIEBChD87J"
      },
      "source": [
        "from jax import jit, vmap, jacrev\n",
        "\n",
        "def gradient_fn(forward_fn, loss_outputs, initial_data):\n",
        "  \"\"\"\n",
        "  This function calls the gradient function, to implement the backpropogation\n",
        "  \"\"\"\n",
        "  boundary_data_x = initial_data['labeled_x']\n",
        "  boundary_data_t = initial_data['labeled_t']\n",
        "\n",
        "  boundary_target = initial_data['labeled_u']\n",
        "\n",
        "  @jax.jit\n",
        "  def model_loss(params, target, weights, rng, x_train, t_train):\n",
        "\n",
        "    @functools.partial(jax.vmap, in_axes=(None, 0, 0))\n",
        "    def small_loss(params, x, t):\n",
        "      u = forward_fn(params, rng, x, t)\n",
        "      u_x, u_t = jacrev(forward_fn, argnums=(2, 3))(params, rng, x, y, z, t)\n",
        "      u_xx = jacrev(jacrev(forward_fn, argnums=2), argnums=2)(params, rng, x, t)\n",
        "      con = 0.01/np.pi\n",
        "      return u_t + u * u_x - con * u_xx\n",
        "\n",
        "    u_pred = forward_fn(params, rng, boundary_data_x, boundary_data_t)\n",
        "    f_pred = small_loss(params, x_train, t_train)\n",
        "    loss_u = jnp.mean((u_pred - boundary_target) ** 2)\n",
        "    loss_f = jnp.mean((f_pred) ** 2)\n",
        "\n",
        "    return loss_u + loss_f\n",
        "\n",
        "  return model_loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg5q7jYs_f-i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzW60NtNpqH9"
      },
      "source": [
        "2. We also need to provide an eval_fn in the below-given format for computing the weights\n",
        "```\n",
        "  >>>\n",
        "  >> def create_eval_fn(forward_fn, params):\n",
        "  >>  def eval_model(..., rng=None):\n",
        "  >>    # write code here using arguments\n",
        "  >>\n",
        "  >>    return\n",
        "  >>  return eval_model\n",
        "```\n",
        "\n",
        "Like previously we have two arguments for our model $(x, t)$ which get passed in function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8etgNQlNpplJ"
      },
      "source": [
        "# Tells the neural network on how to perform calculation during inference\n",
        "def create_eval_fn(forward_fn, params):\n",
        "  \"\"\"\n",
        "  Calls the function to evaluate the model\n",
        "  \"\"\"\n",
        "  @jax.jit\n",
        "  def eval_model(x, t, rng=None):\n",
        "\n",
        "    res = forward_fn(params, rng, x, t)\n",
        "    return jnp.squeeze(res)\n",
        "  return eval_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDOgmtzmD9lp"
      },
      "source": [
        "# Usage of PINN Model\n",
        "\n",
        "We will be using optax library for performing the optimisations. PINNModel executes the codes for training the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOHKt00FEX1H"
      },
      "source": [
        "import optax\n",
        "scheduler = optax.piecewise_constant_schedule(\n",
        "    init_value=1e-2, \n",
        "    boundaries_and_scales={5000: 0.1, 10000: 0.1, 15000: 0.1})\n",
        "\n",
        "opt = optax.chain(\n",
        "    optax.clip_by_global_norm(1.00),\n",
        "    optax.scale_by_adam(b1=0.9, b2=0.99),\n",
        "    optax.scale_by_schedule(scheduler),\n",
        "    optax.scale(-1.0))\n",
        "\n",
        "labeled_x, labeled_t = jnp.split(labeled_X, 2, 1)\n",
        "\n",
        "boundary_data = {\n",
        "    'labeled_x': labeled_x, 'labeled_t':labeled_t, 'labeled_u': labeled_y\n",
        "}\n",
        "\n",
        "  boundary_data_x = initial_data['labeled_x']\n",
        "  boundary_data_t = initial_data['labeled_t']\n",
        "\n",
        "  boundary_target = initial_data['labeled_u']\n",
        "\n",
        "j_m = PINN_Model(      \n",
        "    forward_fn = forward_fn,\n",
        "    params = params,\n",
        "    boundary_data = boundary_data,\n",
        "    batch_size = 1000,\n",
        "    optimizer = opt,\n",
        "    grad_fn = gradient_fn,\n",
        "    eval_fn = create_eval_fn,\n",
        "    deterministic = True,\n",
        "    log_frequency = 1000\n",
        "    )\n",
        "\n",
        "dataset = dc.data.NumpyDataset(unlabeled_X)\n",
        "val = j_m.fit(dataset, nb_epochs=4000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ2tq2dBEYcU"
      },
      "source": [
        "# Visualize the final results\n",
        "\n",
        "- Code taken from authors for visualisation\n",
        "- show both graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wir3ADrnEfBu"
      },
      "source": [
        "test_dataset = dc.data.NumpyDataset(full_domain)\n",
        "u_pred = j_m.predict(test_dataset)\n",
        "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "Error = np.abs(Exact - U_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJT0lSyQ7Q4N"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(9, 5))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
        "cbar = fig.colorbar(h, cax=cax)\n",
        "cbar.ax.tick_params(labelsize=15) \n",
        "\n",
        "ax.plot(\n",
        "    X_u_train[:,1], \n",
        "    X_u_train[:,0], \n",
        "    'kx', label = 'Data (%d points)' % (u_train.shape[0]), \n",
        "    markersize = 4,  # marker size doubled\n",
        "    clip_on = False,\n",
        "    alpha=1.0\n",
        ")\n",
        "\n",
        "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "\n",
        "ax.set_xlabel('$t$', size=20)\n",
        "ax.set_ylabel('$x$', size=20)\n",
        "ax.legend(\n",
        "    loc='upper center', \n",
        "    bbox_to_anchor=(0.9, -0.05), \n",
        "    ncol=5, \n",
        "    frameon=False, \n",
        "    prop={'size': 15}\n",
        ")\n",
        "ax.set_title('$u(t,x)$', fontsize = 20) # font size doubled\n",
        "ax.tick_params(labelsize=15)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(14, 10))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "gs1 = gridspec.GridSpec(1, 3)\n",
        "gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "ax = plt.subplot(gs1[0, 0])\n",
        "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')    \n",
        "ax.set_title('$t = 0.25$', fontsize = 15)\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
        "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "    item.set_fontsize(15)\n",
        "\n",
        "ax = plt.subplot(gs1[0, 1])\n",
        "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "ax.set_title('$t = 0.50$', fontsize = 15)\n",
        "ax.legend(\n",
        "    loc='upper center', \n",
        "    bbox_to_anchor=(0.5, -0.15), \n",
        "    ncol=5, \n",
        "    frameon=False, \n",
        "    prop={'size': 15}\n",
        ")\n",
        "\n",
        "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
        "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "    item.set_fontsize(15)\n",
        "\n",
        "ax = plt.subplot(gs1[0, 2])\n",
        "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])    \n",
        "ax.set_title('$t = 0.75$', fontsize = 15)\n",
        "\n",
        "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
        "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "    item.set_fontsize(15)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}