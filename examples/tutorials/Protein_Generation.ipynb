{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "/home/shivasankaran/miniconda3/envs/deepchem/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch.utils._import_utils'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'MXMNet' from 'deepchem.models.torch_models' (/home/shivasankaran/deepchem/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.models.torch_models.prot_bert import ProtBERT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "from typing import Tuple, List, Any\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "### Tokenizer Wrapper for ProtBERT\n",
    "\n",
    "In this section, we define a wrapper class `ProtBERTTokenizerWrapper` for the ProtBERT tokenizer. This wrapper allows us to conveniently set the maximum sequence length and handle the necessary tokenization steps (like truncation, padding, and conversion to PyTorch tensors) when calling the tokenizer. \n",
    "\n",
    "We use the `AutoTokenizer.from_pretrained()` method to load the pre-trained ProtBERT tokenizer from HuggingFace, and then customize it with additional parameters such as truncation, padding, and max length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary workaround in sending kwargs to HuggingFaceFeaturizer\n",
    "class ProtBERTTokenizerWrapper:\n",
    "    def __init__(self, model_name, max_len=128):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, sequence):\n",
    "        return self.tokenizer(sequence, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "temp_tokenizer = ProtBERTTokenizerWrapper('Rostlab/prot_bert',max_len=512)\n",
    "protbert_featurizer = dc.feat.HuggingFaceFeaturizer(temp_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dc.data.CSVLoader([],\n",
    "                               feature_field=\"protein\",\n",
    "                               featurizer=protbert_featurizer)\n",
    "dataset = loader.create_dataset(\"/home/shivasankaran/deepchem/plastic_degrading_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### 1. Variational Autoencoder (VAE) for Protein Generation\n",
    "\n",
    "In this section, we define a **Variational Autoencoder (VAE)** model, which is designed to generate protein sequences based on a latent space. The architecture of the VAE consists of several key components:\n",
    "\n",
    "- **Latent-to-Hidden Mapping:** A fully connected layer to map the latent vector `z` to a hidden space.\n",
    "- **Positional Embeddings:** An embedding layer to add positional information to the input sequence, enabling the model to learn dependencies across sequence positions.\n",
    "- **Transformer Decoder:** A Transformer decoder layer (with multiple layers) that processes the sequence and latent vector for sequence generation.\n",
    "- **Output Linear Layer:** A fully connected layer that generates token probabilities for the vocabulary.\n",
    "\n",
    "The forward pass uses the latent vector `z` to generate a hidden representation, which is then passed through the Transformer decoder to produce the output sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, vocab_size, num_heads, num_layers, max_seq_len):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "\n",
    "        # Positional embeddings for input sequences\n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, hidden_dim)\n",
    "\n",
    "        # Transformer Decoder components\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=4 * hidden_dim,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        # Linear layer for output token probabilities\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, z, seq_len):\n",
    "        # Map latent vector to the hidden dimension\n",
    "        hidden = self.latent_to_hidden(z)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # Repeat hidden state across sequence length\n",
    "        hidden_seq = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        # Add positional embeddings\n",
    "        positions = torch.arange(seq_len, device=z.device).unsqueeze(0).expand(z.size(0), seq_len)\n",
    "        hidden_seq += self.position_embeddings(positions)  # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        # Transformer decoder requires inputs in shape [seq_len, batch_size, hidden_dim]\n",
    "        hidden_seq = hidden_seq.permute(1, 0, 2)\n",
    "\n",
    "        # Create a causal mask to ensure the decoder only attends to previous tokens\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=z.device), diagonal=1).bool()\n",
    "\n",
    "        # Decode\n",
    "        outputs = self.decoder(\n",
    "            tgt=hidden_seq,\n",
    "            memory=hidden_seq,  # No encoder; latent is used as memory\n",
    "            tgt_mask=causal_mask,\n",
    "        )  # [seq_len, batch_size, hidden_dim]\n",
    "\n",
    "        # Project outputs to vocabulary size\n",
    "        outputs = self.fc_out(outputs.permute(1, 0, 2))  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Protein Generator Model\n",
    "\n",
    "The ProtGenerator class is a higher-level wrapper that integrates the VAE with an encoder and decoder. The encoder processes the input protein sequences, and the decoder generates reconstructed sequences based on the latent representation produced by the encoder. This structure helps in training the model to generate sequences in an unsupervised manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProtGenerator(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(ProtGenerator, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        z = self.encoder.get_feat(input_ids,attention_mask)\n",
    "        recon_x = self.decoder(z, seq_len = input_ids.size(1))\n",
    "        return recon_x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ProtGenerator Wrapper for DeepChem\n",
    "\n",
    "Here, we define the ProtGeneratorDCWrapper class, which wraps the ProtGenerator model into DeepChem's framework. The wrapper integrates a loss function (SparseSoftmaxCrossEntropy) and ensures the model adheres to DeepChem's expected format for training and evaluation.\n",
    "\n",
    "In this wrapper, the model is expected to receive a batch of inputs containing input_ids and attention_mask, and the outputs are processed by the VAE model for protein sequence generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.losses import SparseSoftmaxCrossEntropy\n",
    "class ProtGeneratorDCWrapper(TorchModel):\n",
    "    def __init__(self,model,**kwargs):\n",
    "        self.model = model\n",
    "        loss = SparseSoftmaxCrossEntropy()\n",
    "        output_types = ['loss', 'predict']\n",
    "        super(ProtGeneratorDCWrapper, self).__init__(model,\n",
    "                                      loss=loss,\n",
    "                                      output_types=output_types,\n",
    "                                      **kwargs)\n",
    "    \n",
    "    def _prepare_batch(\n",
    "            self,\n",
    "            batch) -> Tuple[List[Any], List[torch.Tensor], List[torch.Tensor]]:\n",
    "\n",
    "        inputs, labels, _ = batch\n",
    "        inputs = inputs[0]\n",
    "        input_ids = torch.stack([\n",
    "            x['input_ids'][0] \n",
    "            for x in inputs\n",
    "        ])\n",
    "        attention_mask = torch.stack([\n",
    "            x['attention_mask'][0]\n",
    "            for x in inputs\n",
    "        ])\n",
    "        labels = torch.stack([\n",
    "                x['input_ids'][0]\n",
    "                for x in inputs\n",
    "            ])\n",
    "        inputs = {'input_ids':input_ids, \"attention_mask\":attention_mask}\n",
    "        weights = torch.stack([\n",
    "            torch.tensor(1.0)\n",
    "            for x in inputs\n",
    "        ])\n",
    "        return (inputs, [labels], [weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_bert_feat_extractor = ProtBERT(task=\"feature_extractor\",model_path='Rostlab/prot_bert', device = 'cpu')\n",
    "vae_decoder = VAE(latent_dim=prot_bert_feat_extractor.config.hidden_size, hidden_dim=512, vocab_size=prot_bert_feat_extractor.tokenizer.vocab_size,num_heads=8,num_layers=4,max_seq_len=1024 )\n",
    "gen_model_torch = ProtGenerator(prot_bert_feat_extractor,vae_decoder)\n",
    "gen_model_dc = ProtGeneratorDCWrapper(gen_model_torch,device = 'cpu',batch_size = 2,wandb= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_dc.fit(dataset,nb_epoch=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
