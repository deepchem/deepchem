{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Sequence Generation with DeepChem and ProtBERT\n",
    "\n",
    "In this tutorial, we will walk through how to generate protein sequences using pre-trained protein language models (PLMs) integrated into DeepChem. Specifically, we will demonstrate how to use the ProtBERT model—a transformer-based model trained on millions of protein sequences—to generate plausible amino acid sequences conditioned on learned representations.\n",
    "\n",
    "\n",
    "This tutorial is part of DeepChem’s ongoing effort to bring cutting-edge protein language modeling tools to the open-source drug discovery and bioinformatics communities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "#### Install necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre deepchem\n",
    "import deepchem\n",
    "deepchem.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.models.torch_models.prot_bert import ProtBERT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "from typing import Tuple, List, Any\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "from deepchem.data import NumpyDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "### Tokenizer Wrapper for ProtBERT\n",
    "\n",
    "In this section, we define a wrapper class `ProtBERTTokenizerWrapper` for the ProtBERT tokenizer. This wrapper allows us to conveniently set the maximum sequence length and handle the necessary tokenization steps (like truncation, padding, and conversion to PyTorch tensors) when calling the tokenizer. \n",
    "\n",
    "We use the `AutoTokenizer.from_pretrained()` method to load the pre-trained ProtBERT tokenizer from HuggingFace, and then customize it with additional parameters such as truncation, padding, and max length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary workaround in sending kwargs to HuggingFaceFeaturizer\n",
    "class ProtBERTTokenizerWrapper:\n",
    "    def __init__(self, model_name, max_len=128):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, sequence):\n",
    "        return self.tokenizer(sequence, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "temp_tokenizer = ProtBERTTokenizerWrapper('Rostlab/prot_bert',max_len=512)\n",
    "protbert_featurizer = dc.feat.HuggingFaceFeaturizer(temp_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dc.data.CSVLoader([],\n",
    "                               feature_field=\"protein\",\n",
    "                               featurizer=protbert_featurizer)\n",
    "dataset = loader.create_dataset(\"/home/shivasankaran/deepchem/plastic_degrading_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### 1. Variational Autoencoder (VAE) for Protein Generation\n",
    "\n",
    "In this section, we define a **Variational Autoencoder (VAE)** model, which is designed to generate protein sequences based on a latent space. The architecture of the VAE consists of several key components:\n",
    "\n",
    "- **Latent-to-Hidden Mapping:** A fully connected layer to map the latent vector `z` to a hidden space.\n",
    "- **Positional Embeddings:** An embedding layer to add positional information to the input sequence, enabling the model to learn dependencies across sequence positions.\n",
    "- **Transformer Decoder:** A Transformer decoder layer (with multiple layers) that processes the sequence and latent vector for sequence generation.\n",
    "- **Output Linear Layer:** A fully connected layer that generates token probabilities for the vocabulary.\n",
    "\n",
    "The forward pass uses the latent vector `z` to generate a hidden representation, which is then passed through the Transformer decoder to produce the output sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, vocab_size, num_heads, num_layers, max_seq_len):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "\n",
    "        # Positional embeddings for input sequences\n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, hidden_dim)\n",
    "\n",
    "        # Transformer Decoder components\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=4 * hidden_dim,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        # Linear layer for output token probabilities\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, z, seq_len):\n",
    "        # Map latent vector to the hidden dimension\n",
    "        hidden = self.latent_to_hidden(z)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # Repeat hidden state across sequence length\n",
    "        hidden_seq = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        # Add positional embeddings\n",
    "        positions = torch.arange(seq_len, device=z.device).unsqueeze(0).expand(z.size(0), seq_len)\n",
    "        hidden_seq += self.position_embeddings(positions)  # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        # Transformer decoder requires inputs in shape [seq_len, batch_size, hidden_dim]\n",
    "        hidden_seq = hidden_seq.permute(1, 0, 2)\n",
    "\n",
    "        # Create a causal mask to ensure the decoder only attends to previous tokens\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=z.device), diagonal=1).bool()\n",
    "\n",
    "        # Decode\n",
    "        outputs = self.decoder(\n",
    "            tgt=hidden_seq,\n",
    "            memory=hidden_seq,  # No encoder; latent is used as memory\n",
    "            tgt_mask=causal_mask,\n",
    "        )  # [seq_len, batch_size, hidden_dim]\n",
    "\n",
    "        # Project outputs to vocabulary size\n",
    "        outputs = self.fc_out(outputs.permute(1, 0, 2))  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Protein Generator Model\n",
    "\n",
    "The ProtGenerator class is a higher-level wrapper that integrates the VAE with an encoder and decoder. The encoder processes the input protein sequences, and the decoder generates reconstructed sequences based on the latent representation produced by the encoder. This structure helps in training the model to generate sequences in an unsupervised manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProtGenerator(nn.Module):\n",
    "    def __init__(self,encoder,decoder,tokenizer):\n",
    "        super(ProtGenerator, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        z = self.encoder.get_last_hidden_state(input_ids,attention_mask)\n",
    "        recon_x = self.decoder(z, seq_len = input_ids.size(1))\n",
    "        return recon_x\n",
    "\n",
    "    def generate(self,protein_sequence , seq_len, noise_level=0.0, temperature=1.0,exclude_tokens=None):\n",
    "        encoded = self.tokenizer(protein_sequence, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "        input_ids = encoded['input_ids']\n",
    "        attention_mask = encoded['attention_mask']\n",
    "        z = self.encoder.get_last_hidden_state(input_ids,attention_mask)\n",
    "\n",
    "        if noise_level > 0:\n",
    "            noise = torch.randn_like(z) * noise_level  # Generate Gaussian noise\n",
    "            z = z + noise  # Add noise to the latent representation\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.decoder(z, seq_len)  # Shape: (1, seq_len, vocab_size)\n",
    "\n",
    "        outputs = outputs.squeeze(0)  # Remove the batch dimension\n",
    "        generated_sequence = []\n",
    "\n",
    "        for i in range(seq_len):\n",
    "        # Apply softmax to get probabilities, adjusted by temperature\n",
    "            probabilities = F.softmax(outputs[i] / temperature, dim=-1)\n",
    "\n",
    "\n",
    "            # Impose a penalty on [PAD] tokens if a pad_token_id is provided\n",
    "            if exclude_tokens:\n",
    "                for token_id in exclude_tokens:\n",
    "                    # print(token_id)\n",
    "                    probabilities[token_id] = 0\n",
    "\n",
    "            # Normalize probabilities after zeroing out [PAD]\n",
    "\n",
    "            probabilities = probabilities / probabilities.sum()\n",
    "\n",
    "            # Sample a token from the probability distribution\n",
    "            token_index = torch.multinomial(probabilities, num_samples=1).item()\n",
    "\n",
    "            # Append the corresponding amino acid to the generated sequence\n",
    "            token = self.tokenizer.decode([token_index]).strip()\n",
    "            generated_sequence.append(token)\n",
    "\n",
    "        # Join the tokens to form a complete protein sequence, excluding [PAD]\n",
    "        return ' '.join([token for token in generated_sequence if token not in exclude_tokens])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ProtGenerator Wrapper for DeepChem\n",
    "\n",
    "Here, we define the ProtGeneratorDCWrapper class, which wraps the ProtGenerator model into DeepChem's framework. The wrapper integrates a loss function (SparseSoftmaxCrossEntropy) and ensures the model adheres to DeepChem's expected format for training and evaluation.\n",
    "\n",
    "In this wrapper, the model is expected to receive a batch of inputs containing input_ids and attention_mask, and the outputs are processed by the VAE model for protein sequence generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.losses import SparseSoftmaxCrossEntropy\n",
    "import numpy as np\n",
    "class ProtGeneratorDCWrapper(TorchModel):\n",
    "    def __init__(self,model,**kwargs):\n",
    "        self.model = model\n",
    "        loss = SparseSoftmaxCrossEntropy()\n",
    "        output_types = ['loss', 'predict']\n",
    "        super(ProtGeneratorDCWrapper, self).__init__(model,\n",
    "                                      loss=loss,\n",
    "                                      output_types=output_types,\n",
    "                                      **kwargs)\n",
    "\n",
    "    \n",
    "    def predict(self, dataset,seq_len = 512, noise_level=0.0, temperature=1.0, exclude_tokens=None):\n",
    "        \"\"\"\n",
    "        Generates protein sequences for input protein sequences.\n",
    "\n",
    "        Args:\n",
    "            dataset (deepchem.Dataset): DeepChem dataset containing input sequences.\n",
    "            noise_level (float): Noise to add to latent representations.\n",
    "            temperature (float): Sampling temperature.\n",
    "            exclude_tokens (list): Tokens to exclude from generated sequences.\n",
    "\n",
    "        Returns:\n",
    "            list: Generated protein sequences.\n",
    "        \"\"\"\n",
    "        inputs = dataset.X  # Assuming dataset contains raw protein sequences\n",
    "        generated_sequences = []\n",
    "\n",
    "        for protein_sequence in inputs:\n",
    "            generated_seq = self.model.generate(\n",
    "                protein_sequence,\n",
    "                seq_len=seq_len,  # Adjust as needed\n",
    "                noise_level=noise_level,\n",
    "                temperature=temperature,\n",
    "                exclude_tokens=exclude_tokens\n",
    "            )\n",
    "            generated_sequences.append(generated_seq)\n",
    "\n",
    "        return np.array(generated_sequences)\n",
    "    \n",
    "    def _prepare_batch(\n",
    "            self,\n",
    "            batch) -> Tuple[List[Any], List[torch.Tensor], List[torch.Tensor]]:\n",
    "\n",
    "        inputs, labels, _ = batch\n",
    "        inputs = inputs[0]\n",
    "        input_ids = torch.stack([\n",
    "            x['input_ids'][0] \n",
    "            for x in inputs\n",
    "        ])\n",
    "        attention_mask = torch.stack([\n",
    "            x['attention_mask'][0]\n",
    "            for x in inputs\n",
    "        ])\n",
    "        labels = torch.stack([\n",
    "                x['input_ids'][0]\n",
    "                for x in inputs\n",
    "            ])\n",
    "        inputs = {'input_ids':input_ids, \"attention_mask\":attention_mask}\n",
    "        weights = torch.stack([\n",
    "            torch.tensor(1.0)\n",
    "            for x in inputs\n",
    "        ])\n",
    "        return (inputs, [labels], [weights])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`wandb` argument is deprecated. Please use `wandb_logger` instead. This argument will be removed in a future release of DeepChem.\n",
      "You set wandb to True but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert')\n",
    "prot_bert_feat_extractor = ProtBERT(task=\"feature_extractor\",model_path='Rostlab/prot_bert', device = 'cpu')\n",
    "vae_decoder = VAE(latent_dim=prot_bert_feat_extractor.config.hidden_size, hidden_dim=512, vocab_size=prot_bert_feat_extractor.tokenizer.vocab_size,num_heads=8,num_layers=4,max_seq_len=1024 )\n",
    "gen_model_torch = ProtGenerator(prot_bert_feat_extractor,vae_decoder, tokenizer)\n",
    "gen_model_dc = ProtGeneratorDCWrapper(gen_model_torch,device = 'cpu',batch_size = 2,wandb= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Protein Generation Model\n",
    "\n",
    "Now that we've constructed the model and loaded our protein dataset, it's time to train the model. We'll run a single epoch to demonstrate the training loop and ensure everything is working end-to-end.\n",
    "\n",
    "This step will fine-tune the ProtBERT-based generator on the provided dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_dc.fit(dataset,nb_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Protein Sequences\n",
    "\n",
    "With the model trained, we can now generate new protein sequences. These sequences are sampled based on the learned representations from the fine-tuned ProtBERT model.\n",
    "\n",
    "This step demonstrates how the model can be used to create plausible protein-like sequences that follow the patterns learned from the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = [\n",
    "    \"M S A S P R L G F V Q C I S P A G L H R M A Y H E W G D P A N P R V L V C A H G L T R G R D F D T V A S A L C G D Y R V V C P D V A G R G R S E W L A D A N G Y V V P Q Y V S D M V T L I A R L N V E K V D W F G T S M G G L I G M G L A G L P K S P V R N V L L N D V G P K L A P S A V E R I G A Y L G L P V R F K T F E E G L A Y L Q T I S A S F G R H T P E Q W R E L N A A I L K P V Q G T D G L E W G L H Y D P Q L A V P F R K S T P E A I A A G E A A L W R T F E A I E G P V L V V R G A Q S D L L L R E T V A E M V A R G K H V S S V E V P D V G H A P T F V D P A Q I A I A P Q F F T G A\",\n",
    "    \"M L A K Q I K K A N S R S T L L R K S L L F A A P I I L A V S S S S V Y A L T Q V S N F G T N P G N L Q M F K H V P S G M P A N A P L V V A L H G C T Q T A A A Y E A S G W S A L G N T H K F Y V V Y P Q Q Q S G N N S N K C F N W F E P G D I T R G Q G E A L S I K Q M V D N M K A N H S I D P S R V Y V T G L S A G A F M T T V M A A T Y P D V F A G A A P I A G G P Y K C A T S M T S A F T C M S P G V D K T P A A W G D L A R G G Y S G Y N G P K P K I S I W H G S S D Y T V A P A N Q N E T V E Q F T N Y H G I D Q T P D V S D T V G G F P H K V Y K S A N G T P L V E T Y T I T G M G H G T P V D P G T G A N Q C G T A G A Y I L D V N V C S S Y Y I G Q F F G I I G G G G T T T T T T S G N V T T T T A A T T T T T T A T Q G Y T Q T T S A T V T N H Y V A G R I N V T Q Y N V L G A R Y G Y V T T I P L Y Y C P S L S G W T D K A N C S P I\"\n",
    "]\n",
    "\n",
    "dc_dataset = NumpyDataset(np.array(input_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M M P S L A A A A A A L A G P A G G P G A P P A A A A A G A T A P A G G P A A G P G S A A S A A A L G P L A P G A A G P A G P A G A G A A A G S A A G L A A P A A A A A G A A A A S A S A A P V A G A T A A A A G A P A P A A A A A V A A G A G P P A P A S A T L G A A G L A G A A A A A G A G P G G G S A A A A A P L S D P A A A A A A P A A A S A A A A L P A T A A G V S G G P T G A A P A P G A A G G A A G A A A G A A A T V P A A G P G S A G A G A G A A G L A G A A L G A R A A G A G A G A G A A G A A A A V G G A T A G A G A G A A G G G A G G R A G A S A A V G A P A A A G A R A G G G A S V G G A A G A',\n",
       "       'S M D R P V F N L I S P T N L V P G T G H V D S S R P T G P S F N R S V G N T L A N D A S Y F L V R A G A T G N P T Y D K G T V G T G L I V Q R T S V K P A V Q T T S F S G D Y P G F E A L V L S E N N S A E N T T E S A V I I N L A L S S C A I K I T P D V T T D G L S N E V L V Q G P Q G A S S Q S P N E L L L G G M F L S V D G L R L G A I N G G I E G F V G T V Q P A E V S P G I G G T Q S S A A N N A P W N D C D N S V A K L A S G G P S A L S A G T R L T F R G I F Y G P N S N D Y R G V D F S S L Q T D A A E V M L G T P D L R G D V P L S S G I I G D G G Q G T Q V Q N A G Y S Q A S F R T Y N E V I F'],\n",
       "      dtype='<U599')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "seq_len = 300  # Length of the desired generated sequence\n",
    "temperature = 1  # Adjust temperature for randomness control\n",
    "noise_level = 0.1\n",
    "protein_seed = \"M S A S P R L G F V Q C I S P A G L H R M A Y H E W G D P A N P R V L V C A H G L T R G R D F D T V A S A L C G D Y R V V C P D V A G R G R S E W L A D A N G Y V V P Q Y V S D M V T L I A R L N V E K V D W F G T S M G G L I G M G L A G L P K S P V R N V L L N D V G P K L A P S A V E R I G A Y L G L P V R F K T F E E G L A Y L Q T I S A S F G R H T P E Q W R E L N A A I L K P V Q G T D G L E W G L H Y D P Q L A V P F R K S T P E A I A A G E A A L W R T F E A I E G P V L V V R G A Q S D L L L R E T V A E M V A R G K H V S S V E V P D V G H A P T F V D P A Q I A I A P Q F F T G A\"\n",
    "exclude_tokens = [\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.cls_token_id,\n",
    "    tokenizer.mask_token_id,\n",
    "    tokenizer.unk_token_id,\n",
    "    tokenizer.sep_token_id\n",
    "]\n",
    "gen_model_dc.predict(dc_dataset,seq_len,noise_level,temperature,exclude_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "The predicted 3D structure of the generated protein was obtained using AlphaFold. The model provides per-residue confidence scores using the predicted Local Distance Difference Test (pLDDT), which indicates the reliability of the predicted atomic positions. The confidence is visualized using the following color scheme:\n",
    "\n",
    "- **Blue** (`pLDDT > 90`)\n",
    "- **Teal** (`70 < pLDDT ≤ 90`)\n",
    "- **Yellow** (`50 < pLDDT ≤ 70`)\n",
    "- **Orange** (`pLDDT ≤ 50`)\n",
    "\n",
    "These colors help identify regions of the protein structure with high to low confidence, making it easier to interpret which parts of the structure are predicted with greater certainty.\n",
    "\n",
    "\n",
    "\n",
    "![alternate image name](./assets/sample_generated_protein.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! Time to join the Community!\n",
    "Congratulations on completing this tutorial notebook! If you enjoyed working through the tutorial, and want to continue working with DeepChem, we encourage you to finish the rest of the tutorials in this series. You can also help the DeepChem community in the following ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star DeepChem on [GitHub](https://github.com/deepchem/deepchem)\n",
    "This helps build awareness of the DeepChem project and the tools for open source drug discovery that we're trying to build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the DeepChem Discord\n",
    "The DeepChem [Discord](https://discord.gg/cGzwCdrUqS) hosts a number of scientists, developers, and enthusiasts interested in deep learning for the life sciences. Join the conversation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citing this Tutorial\n",
    "\n",
    "If you use this tutorial or the underlying tools in your work, please consider citing our paper:\n",
    "```\n",
    "@article{vanaja2024open,\n",
    "  title={Open-Source Protein Language Models for Function Prediction and Protein Design},\n",
    "  author={Vanaja Pandi, Shivasankaran and Ramsundar, Bharath},\n",
    "  journal={arXiv e-prints},\n",
    "  pages={arXiv--2412},\n",
    "  year={2024}\n",
    "}\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
