{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Differentiation Infrastructure in Deepchem**\n",
    "\n",
    "Author : Rakshit Kr. Singh : [Website](https://greatrsingh.in/) : [LinkedIn](https://www.linkedin.com/in/rakshit-singh-ai/) : [GitHub](https://github.com/GreatRSingh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scientific advancement in machine learning hinges on the effective resolution of complex optimization problems. From material property design to drug discovery, these problems often involve numerous variables and intricate relationships. Traditional optimization techniques often face hurdles when addressing such challenges, often resulting in slow convergence or solutions deemed unreliable. We introduce solutions that are differentiable and also seamlessly integrable into machine learning systems, offering a novel approach to resolving these complexities.\n",
    "\n",
    "This tutorials introduces DeepChem's comprehensive set of differentiable optimisation tools to empower researchers across the physical sciences. DeepChem addresses limitations of conventional methods by offering a diverse set of optimization algorithms. These includes established techniques like Broyden's first and second methods alongside cutting-edge advancements, allowing researchers to select the most effective approach for their specific problem.\n",
    "\n",
    "## Colab\n",
    "\n",
    "This tutorial and the rest in this sequence are designed to be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/Differentiation_Infrastructure_in_Deepchem.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Differentiation Utilities in Deepchem\n",
    "\n",
    "DeepChem provides a number of optimisation algorithms and Utilities for implementing more algorithms. Some of the optimisation algorithms provided by DeepChem are:\n",
    "- Broyden's First Method\n",
    "- Broyden's Second Method\n",
    "- Anderson Acceleration\n",
    "- Gradient Descent\n",
    "- Adam\n",
    "\n",
    "Along with these optimisation algorithms, DeepChem also provides a number of utilities for implementing more algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Non Linear Equations? and why do they matter?\n",
    "\n",
    "Nonlinear equations are mathematical expressions where the relationship between the variables is not linear. Unlike linear equations, which have a constant rate of change, nonlinear equations involve terms with higher powers or functions like exponentials, logarithms, trigonometric functions, etc.\n",
    "\n",
    "Nonlinear equations are essential across various disciplines, including physics, engineering, economics, biology, and finance. They describe complex relationships and phenomena that cannot be adequately modeled with linear equations. From gravitational interactions in celestial bodies to biochemical reactions in living organisms, non-linear equations play a vital role in understanding and predicting real-world systems. Whether it's optimizing structures, analyzing market dynamics, or designing machine learning algorithms.\n",
    "\n",
    "### Some Simple Non Linear Equations:\n",
    "\n",
    "$f(x) = sin(x)$, is a trigonometric function defined for all real numbers.\n",
    "It represents the ratio of the length of the side opposite an angle in a right triangle to the length of the hypotenuse.\n",
    "\n",
    "$f(x) = cos(x)$, is an another trigonometric function.\n",
    "It represents the ratio of the length of the adjacent side of a right triangle to the length of the hypotenuse when x is the measure of an acute angle.\n",
    "\n",
    "$f(x) = x^2$, is a parabola, symmetric around the y-axis, with its vertex at the origin.\n",
    "It represents a mathematical model of quadratic growth or decay. In physical systems, it often describes phenomena where the rate of change is proportional to the square of the quantity involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0, 10, 100)\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "y3 = x**2\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))  \n",
    "\n",
    "axs[0].plot(x, y1, color='blue')\n",
    "axs[0].set_title('Sin')\n",
    "\n",
    "axs[1].plot(x, y2, color='red')\n",
    "axs[1].set_title('Cos')\n",
    "\n",
    "axs[2].plot(x, y3, color='green')\n",
    "axs[2].set_title('x^2')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Finder Methods\n",
    "\n",
    "`deepchem.utils.differentiation_utils.optimize.rootfinder` provides a collection of algorithms for solving nonlinear equations. These methods are designed to find the roots of functions efficiently, making them indispensable for a wide range of applications in mathematics, physics, engineering, and other fields.\n",
    "\n",
    "At its core, rootfinding seeks to determine the solutions (roots) of equations, where a function equals zero. This operation plays a pivotal role in numerous real-world applications, making it indispensable in both theoretical and practical domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broyden's First Method:\n",
    "\n",
    "Broyden's First Method is an iterative numerical method used for solving systems of nonlinear equations. It's particularly useful when the Jacobian matrix (the matrix of partial derivatives of the equations) is difficult or expensive to compute. \n",
    "\n",
    "Broyden's Method is an extension of the Secant Method for systems of nonlinear equations. It iteratively updates an approximation to the Jacobian matrix using the information from previous iterations. The algorithm converges to the solution by updating the variables in the direction that minimizes the norm of the system of equations.\n",
    "\n",
    "Steps:\n",
    "1. Initialize the approximation to the Jacobian matrix.\n",
    "$$J_{k}$$\n",
    "\n",
    "2. Initialize the variables.\n",
    "$$X_{k} = f(X_{k})$$\n",
    "\n",
    "3. Compute the function values.\n",
    "$$f_{k} = f(X_{k})$$\n",
    "\n",
    "4. Update the variables.\n",
    "$$X_{k+1} = X_{k+1} - J_{k}^{-1}f_k$$\n",
    "\n",
    "5. Compute the change in variables.\n",
    "$$\\Delta X_{k+1} = X_{k+1} - X_{k}$$\n",
    "\n",
    "6. Compute the function values.\n",
    "$$f_{k+1} = f(X_{k+1})$$\n",
    "\n",
    "7. Update the approximation to the Jacobian matrix.\n",
    "$$J^{-1}_{k+1} = J^{-1}_{k} + \\frac{(\\Delta X_{k+1} - J^{-1}_{k} \\Delta f_{k+1}) \\Delta X^{T}_{k+1} J^{-1}_{k}}{\\Delta X^{T}_{k+1} J^{-1}_{k} \\Delta f_{k+1}}$$\n",
    "\n",
    "8. Repeat steps 4-7 until convergence criteria are met.\n",
    "\n",
    "#### References:\n",
    "\n",
    "[1] \"A class of methods for solving nonlinear simultaneous equations\" by Charles G. Broyden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepchem.utils.differentiation_utils import rootfinder\n",
    "def func1(y, A):\n",
    "    return torch.tanh(A @ y + 0.1) + y / 2.0\n",
    "A = torch.tensor([[1.1, 0.4], [0.3, 0.8]]).requires_grad_()\n",
    "y0 = torch.zeros((2,1))\n",
    "\n",
    "# Broyden's First Method\n",
    "yroot = rootfinder(func1, y0, params=(A,), method='broyden1')\n",
    "print(\"Root By Broyden's First Method:\")\n",
    "print(yroot)\n",
    "print(\"Function Value at Calculated Root:\")\n",
    "print(func1(yroot, A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.utils.differentiation_utils.optimize.rootsolver import broyden1\n",
    "def fcn(x):\n",
    "   return x**2 - 4 + torch.tan(x)\n",
    "x0 = torch.tensor(0.0, requires_grad=True)\n",
    "x = broyden1(fcn, x0)\n",
    "x, fcn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broyden's Second Method:\n",
    "\n",
    "Broyden's Second Method differs from the first method in how it updates the approximation to the Jacobian matrix. Instead of using the change in variables and function values, it uses the change in the residuals (the difference between the function values and the target values) to update the Jacobian matrix. This approach can be more stable and robust in certain situations.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1...6 are same as Broyden's First Method.\n",
    "\n",
    "7. Update the approximation to the Jacobian matrix.\n",
    "$$J^{-1}_{k+1} = J^{-1}_{k} + \\frac{(\\Delta X_{k+1} - J^{-1}_{k} \\Delta f_{k+1}) \\Delta f^{T}_{k+1}}{\\Delta f^{T}_{k+1} \\Delta f_{k+1}}$$\n",
    "\n",
    "8. Repeat steps 4-7 until convergence criteria are met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broyden's Second Method\n",
    "import torch\n",
    "from deepchem.utils.differentiation_utils import rootfinder\n",
    "def func1(y, A):\n",
    "    return torch.tanh(A @ y + 0.1) + y / 2.0\n",
    "A = torch.tensor([[1.1, 0.4], [0.3, 0.8]]).requires_grad_()\n",
    "y0 = torch.zeros((2,1))\n",
    "\n",
    "yroot = rootfinder(func1, y0, params=(A,), method='broyden2')\n",
    "print(\"\\nRoot by Broyden's Second Method:\")\n",
    "print(yroot)\n",
    "print(\"Function Value at Calculated Root:\")\n",
    "print(func1(yroot, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equilibrium Methods (Fixed Point Iteration)\n",
    "\n",
    "`deepchem.utils.differentiation_utils.optimize.equilibrium` contains algorithms for solving equilibrium problems, where the goal is to find a fixed point of a function. While all the rootfinding methods can be used to solve equilibrium problems, these specialized algorithms are designed to exploit the structure of equilibrium problems for more efficient convergence.\n",
    "\n",
    "Equilibrium methods are essential in machine learning for optimizing models, ensuring stability and convergence, regularizing parameters, and analyzing strategic interactions in multi-agent systems. By leveraging equilibrium principles and techniques, machine learning practitioners can train more robust and generalizable models capable of addressing a wide range of real-world challenges.\n",
    "\n",
    "### The Fixed-Point Problem:\n",
    "Given the function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$, compute a fixed-point $x^{*} \\in \\mathbb{R}^{n}$ such that\n",
    "$$x^{*} = f(x^{*})$$\n",
    "\n",
    "### Classical Approach:\n",
    "Steps:\n",
    "1. Initialize the variables. $x_{k}$\n",
    "2. Compute the function values. $f_{k} = f(x_{k})$\n",
    "3. Update the variables. $x_{k+1} = f_{k}$\n",
    "4. Repeat steps 2-3 until convergence criteria are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson Acceleration Approach (Anderson Mixing):\n",
    "Anderson Acceleration is an iterative method for accelerating the convergence of fixed-point \n",
    "iterations. It combines information from previous iterations to construct a better approximation to \n",
    "the fixed-point. The algorithm uses a history of function values and updates to compute a new \n",
    "iterate that minimizes the residual norm.\n",
    "\n",
    "Steps:\n",
    "1. $\\textbf{Initialize point: } x_{0}$, fixed-point mapping $f : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$\n",
    "\n",
    "2. $\\textbf{for}$ $k = 0, 1,... $ $\\textbf{do}$\n",
    "   - Choose $m_{k}$ (e.g., $m_{k} = min(m, k)$  for some integer $m \\geq 0$).\n",
    "   - Select weights $\\alpha_{k}^{j}$ based on the last $m_{k}$ iterations satisfying $\\sum_{j=0}^{m_{k}} \\alpha_{j}^{k} = 1$.\n",
    "   - $x_{k+1} = \\sum_{j=0}^{m_{k}} \\alpha_{j}^{k}f(x_{k-m_{k}+j})$.\n",
    "3. $\\textbf{end}$ $\\textbf{for}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from deepchem.utils.differentiation_utils.optimize.equilibrium import anderson_acc\n",
    "x_value, f_value = [], []\n",
    "def fcn(x, a):\n",
    "    x_value.append(x.item())\n",
    "    f_value.append((a/x + x).item()/2)\n",
    "    return (a/x + x)/2\n",
    "a = 2.0\n",
    "x0 = torch.tensor([1.0], requires_grad=True)\n",
    "x = anderson_acc(fcn, x0, params=[a], maxiter=16)\n",
    "print(\"Root by Anderson Acceleration:\", x.item())\n",
    "print(\"Function Value at Calculated Root:\", fcn(x, a).item())\n",
    "\n",
    "# Plotting the convergence of Anderson Acceleration\n",
    "plt.plot(x_value, label='Input Value')\n",
    "plt.plot(f_value, label='Func. Value by Anderson Acce.')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Function Value')\n",
    "plt.title('Convergence of Anderson Acceleration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizer\n",
    "\n",
    "`deepchem.utils.differentiation_utils.optimize.minimizer` provides a collection of algorithms for minimizing functions. These methods are designed to find the minimum of a function efficiently, making them indispensable for a wide range of applications in mathematics, physics, engineering, and other fields.\n",
    "\n",
    "Minimization algorithms, including variants of gradient descent like ADAM, are fundamental tools in various fields of science, engineering, and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function.\n",
    "\n",
    "It is used to minimize the cost function in various machine learning and optimization problems. It iteratively updates the parameters in the direction of the negative gradient of the cost function.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. $\\textbf{Initialize Parameters: }$ Denote the parameter vector to be optimized - $\\theta$ and $\\theta_0$ represent the initial guess.\n",
    "    \n",
    "2. $\\textbf{Compute Gradient: }$ Calculate the gradient of the cost function $J(\\theta)$ with respect to each parameter.\n",
    "\n",
    "   $$\\nabla J(\\theta) = \\left[ \\frac{\\partial J(\\theta)}{\\partial \\theta_1}, \\frac{\\partial J(\\theta)}{\\partial \\theta_2}, \\ldots, \\frac{\\partial J(\\theta)}{\\partial \\theta_n} \\right]^T$$\n",
    "\n",
    "   - $\\textbf{Update Parameters: }$ Adjust the parameters in the opposite direction of the gradient to minimize the cost function according to the learning rate $\\alpha$:\n",
    "\n",
    "   $$\\theta = \\theta - \\alpha \\nabla J(\\theta)$$\n",
    "    \n",
    "   - $\\textbf{Repeat: }$ Steps 2 and 3 until the algorithm converges or Stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepchem.utils.differentiation_utils.optimize.minimizer import gd\n",
    "def fcn(x):\n",
    "    return 2 * x + (x - 2) ** 2, 2 * (x - 2) + 2\n",
    "x0 = torch.tensor(0.0, requires_grad=True)\n",
    "x = gd(fcn, x0, [])\n",
    "print(\"Minimum by Gradient Descent:\", x.item())\n",
    "print(\"Function Value at Calculated Minimum:\", fcn(x)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM (Adaptive Moment Estimation)\n",
    "\n",
    "ADAM is an optimization algorithm used for training deep learning models. It's an extension of the gradient descent optimization algorithm and combines the ideas of both momentum and RMSProp algorithms.\n",
    "\n",
    "Steps:\n",
    "1. $\\textbf{Initialization: }$ ADAM initializes two moving average variables: $m$ (the first moment, similar to momentum) and $v$ (the second moment, similar to RMSProp).\n",
    "    \n",
    "2. $\\textbf{Compute Gradients: }$ At each iteration of training, the gradients of the parameters concerning the loss function are computed.\n",
    "\n",
    "3. $\\textbf{Update Moving Averages: }$ The moving averages $m$ and $v$ are updated using exponential decay, with momentum and RMSProp components respectively:\n",
    "\n",
    "$$m_{t} = \\beta_{1} m_{t-1} + (1 - \\beta_{1}) \\nabla J(\\theta_{t})$$\n",
    "$$v_{t} = \\beta_{2} v_{t-1} + (1 - \\beta_{2}) (\\nabla J(\\theta_{t}))^2$$\n",
    "\n",
    "4. $\\textbf{Bias Correction: }$ Due to the initialization of the moving averages to zero vectors, there's a bias towards zero, especially during the initial iterations. To correct this bias, ADAM applies a bias correction step:\n",
    "\n",
    "$$\\hat{m}_{t} = \\frac{m_{t}}{1 - \\beta_{1}^t}$$\n",
    "$$\\hat{v}_{t} = \\frac{v_{t}}{1 - \\beta_{2}^t}$$\n",
    "\n",
    "5. $\\textbf{Update Parameters: }$ Finally, the parameters (weights and biases) of the model are updated using the moving averages and the learning rate $\\alpha$:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\alpha \\frac{\\hat{m}_{t}}{\\sqrt{\\hat{v}_{t}} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepchem.utils.differentiation_utils.optimize.minimizer import adam\n",
    "def fcn(x):\n",
    "    return 2 * x + (x - 2) ** 2, 2 * (x - 2) + 2\n",
    "x0 = torch.tensor(10.0, requires_grad=True)\n",
    "x = adam(fcn, x0, [], maxiter=20000)\n",
    "print(\"X at Minimum by Adam:\", x.item())\n",
    "print(\"Function Value at Calculated Minimum:\", fcn(x)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Differentiable optimization techniques are essential for many advanced computational experiments involving Environment Simulations like DFT, Physics Informed Neural Networks and as fundamental mathematical foundation for Molecular Simulation like Monte Carlo and Molecular Dynamics.\n",
    "\n",
    "By integrating deep learning into simulations, we optimize efficiency and accuracy by leveraging trainable neural networks to replace costly or less precise components. This advancement holds immense potential for expediting scientific advancements and addressing longstanding mysteries with greater efficacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Raissi M, Perdikaris P, Karniadakis GE. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics (2019)\n",
    "\n",
    "[2] Muhammad F. Kasim, Sam M. Vinko. Learning the exchange-correlation functional from nature with fully differentiable density functional theory. 2021 American Physical Society\n",
    "\n",
    "[3] Nathan Argaman, Guy Makov. Density Functional Theory -- an introduction. American Journal of Physics 68 (2000), 69-79\n",
    "\n",
    "[4] John Ingraham et al. Learning Protein Structure with a Differentiable Simulator. ICLR.\n",
    "2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citing This Tutorial\n",
    "\n",
    "If you found this tutorial useful please consider citing it using the provided BibTeX.\n",
    "\n",
    "```bibtex\n",
    "@manual{Quantum Chemistry, \n",
    " title={Differentiation Infrastructure in Deepchem}, \n",
    " organization={DeepChem},\n",
    " author={Singh, Rakshit kr.},\n",
    " howpublished = {\\url{https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Differentiation_Infrastructure_in_Deepchem.ipynb}}, \n",
    " year={2024}, \n",
    "} \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! Time to join the Community!\n",
    "\n",
    "Congratulations on completing this tutorial notebook! If you enjoyed working through the tutorial, and want to continue working with DeepChem, we encourage you to finish the rest of the tutorials in this series. You can also help the DeepChem community in the following ways:\n",
    "\n",
    "## Star DeepChem on [GitHub](https://github.com/deepchem/deepchem)\n",
    "This helps build awareness of the DeepChem project and the tools for open source drug discovery that we're trying to build.\n",
    "\n",
    "\n",
    "## Join the DeepChem Discord\n",
    "The DeepChem [Discord](https://discord.gg/SxSzjRRDMA) hosts a number of scientists, developers, and enthusiasts interested in deep learning for the life sciences. Join the conversation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
