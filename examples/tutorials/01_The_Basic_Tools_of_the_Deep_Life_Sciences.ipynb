{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "01_The_Basic_Tools_of_the_Deep_Life_Sciences.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "socSJe925zFv",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial 1: The Basic Tools of the Deep Life Sciences\n",
        "Welcome to DeepChem's introductory tutorial for the deep life sciences. This series of notebooks is step-by-step guide for you to get to know the new tools and techniques needed to do deep learning for the life sciences. We'll start from the basics, assuming that you're new to machine learning and the life sciences, and build up a repertoire of tools and techniques that you can use to do meaningful work in the life sciences.\n",
        "\n",
        "**Scope:** This tutorial will encompass both the machine learning and data handling needed to build systems for the deep life sciences.\n",
        "\n",
        "## Colab\n",
        "\n",
        "This tutorial and the rest in the sequences are designed to be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/01_The_Basic_Tools_of_the_Deep_Life_Sciences.ipynb)\n",
        "\n",
        "\n",
        "## Why do the DeepChem Tutorial?\n",
        "\n",
        "**1) Career Advancement:** Applying AI in the life sciences is a booming\n",
        "industry at present. There are a host of newly funded startups and initiatives\n",
        "at large pharmaceutical and biotech companies centered around AI. Learning and\n",
        "mastering DeepChem will bring you to the forefront of this field and will\n",
        "prepare you to enter a career in this field.\n",
        "\n",
        "**2) Humanitarian Considerations:** Disease is the oldest cause of human\n",
        "suffering. From the dawn of human civilization, humans have suffered from pathogens,\n",
        "cancers, and neurological conditions. One of the greatest achievements of\n",
        "the last few centuries has been the development of effective treatments for\n",
        "many diseases. By mastering the skills in this tutorial, you will be able to\n",
        "stand on the shoulders of the giants of the past to help develop new\n",
        "medicine.\n",
        "\n",
        "**3) Lowering the Cost of Medicine:** The art of developing new medicine is\n",
        "currently an elite skill that can only be practiced by a small core of expert\n",
        "practitioners. By enabling the growth of open source tools for drug discovery,\n",
        "you can help democratize these skills and open up drug discovery to more\n",
        "competition. Increased competition can help drive down the cost of medicine.\n",
        "\n",
        "## Getting Extra Credit\n",
        "If you're excited about DeepChem and want to get more more involved, there's a couple of things that you can do right now:\n",
        "\n",
        "* Star DeepChem on GitHub! - https://github.com/deepchem/deepchem\n",
        "* Join the DeepChem forums and introduce yourself! - https://forum.deepchem.io\n",
        "* Say hi on the DeepChem gitter - https://gitter.im/deepchem/Lobby\n",
        "* Make a YouTube video teaching the contents of this notebook.\n",
        "\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This tutorial will assume some basic familiarity with the Python data science ecosystem. We will assume that you have familiarity with libraries such as Numpy, Pandas, and TensorFlow. We'll provide some brief refreshers on basics through the tutorial so don't worry if you're not an expert.\n",
        "\n",
        "## Setup\n",
        "\n",
        "The first step is to get DeepChem up and running. We recommend using Google Colab to work through this tutorial series. You'll need to run the following commands to get DeepChem installed on your colab notebook. Note that this will take something like 5 minutes to run on your colab instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyxRVW5X5zF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "affd23f1-1929-456a-f8a6-e53a874c84b4"
      },
      "source": [
        "!curl -Lo conda_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
        "import conda_installer\n",
        "conda_installer.install()\n",
        "!/root/miniconda/bin/conda info -e"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3489  100  3489    0     0  39202      0 --:--:-- --:--:-- --:--:-- 39202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "add /root/miniconda/lib/python3.6/site-packages to PYTHONPATH\n",
            "all packages is already installed\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "# conda environments:\n",
            "#\n",
            "base                  *  /root/miniconda\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMWAv-Z46nCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "9ae7cfd0-ebbf-40b0-f6f1-2940cf32a839"
      },
      "source": [
        "!pip install --pre deepchem"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deepchem in /usr/local/lib/python3.6/dist-packages (2.4.0rc1.dev20200805140059)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from deepchem) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from deepchem) (0.22.2.post1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepchem) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->deepchem) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->deepchem) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk47QTZ95zF-",
        "colab_type": "text"
      },
      "source": [
        "You can of course run this tutorial locally if you prefer. In this case, don't run the above cell since it will download and install Anaconda on your local machine. In either case, we can now import `deepchem` the package to play with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDiY03h35zF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cdd7401d-19a0-4476-9297-b04defc67178"
      },
      "source": [
        "# Run this cell to see if things work\n",
        "import deepchem as dc\n",
        "dc.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0-rc1.dev'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0u7qIZd5zGG",
        "colab_type": "text"
      },
      "source": [
        "# Basic Data Handling in DeepChem\n",
        "What does it take to do deep learning on the life sciences? Well, the first thing we'll need to do is actually handle some data. How can we start handling some basic data? For beginners, let's just take a look at some synthetic data.\n",
        "\n",
        "To generate some basic synthetic data, we will use Numpy to create some basic arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saTaOpXY5zGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.random.random((4, 4))\n",
        "labels = np.random.random((4,)) # labels of size 20x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F922OPtL5zGM",
        "colab_type": "text"
      },
      "source": [
        "We've given these arrays some evocative names: \"data\" and \"labels.\" For now, don't worry too much about the names, but just note that the arrays have different shapes. Let's take a quick look to get a feeling for these arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEDcUsz35zGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5a05747f-8b06-407d-9b11-790a1b4d1c8f"
      },
      "source": [
        "data, labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.98945421, 0.63065257, 0.30835689, 0.87841894],\n",
              "        [0.88537488, 0.24523746, 0.12397733, 0.00886653],\n",
              "        [0.11237206, 0.02017302, 0.74253676, 0.86894009],\n",
              "        [0.43141617, 0.73671167, 0.35075885, 0.26500112]]),\n",
              " array([0.05286423, 0.36045732, 0.91513713, 0.02466782]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8UCFrrN5zGf",
        "colab_type": "text"
      },
      "source": [
        "In order to be able to work with this data in DeepChem, we need to wrap these arrays so DeepChem knows how to work with them. DeepChem has a `Dataset` API that it uses to facilitate its handling of datasets. For handling of Numpy datasets, we use DeepChem's `NumpyDataset` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5K3rdGV5zGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from deepchem.data.datasets import NumpyDataset\n",
        "\n",
        "dataset = NumpyDataset(data, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zcd7jTd5zGr",
        "colab_type": "text"
      },
      "source": [
        "Ok, now what? We have these arrays in a `NumpyDataset` object. What can we do with it? Let's try printing out the object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJc90fs_5zGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c9fd5ab-e23a-40dc-9292-8b4ff3a86890"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NumpyDataset X.shape: (4, 4), y.shape: (4,), w.shape: (4,), ids: [0 1 2 3], task_names: [0]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQa88cbj5zGw",
        "colab_type": "text"
      },
      "source": [
        "Ok, that's not terribly informative. It's telling us that `dataset` is a Python object that lives somewhere in memory. Can we recover the two datasets that we used to construct this object? Luckily, the DeepChem API allows us to recover the two original datasets by calling the `dataset.X` and `dataset.y` attributes of the original object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSVqeYox5zGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "270a6a17-6238-4081-b0cf-3f17e23f4bb5"
      },
      "source": [
        "dataset.X, dataset.y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.98945421, 0.63065257, 0.30835689, 0.87841894],\n",
              "        [0.88537488, 0.24523746, 0.12397733, 0.00886653],\n",
              "        [0.11237206, 0.02017302, 0.74253676, 0.86894009],\n",
              "        [0.43141617, 0.73671167, 0.35075885, 0.26500112]]),\n",
              " array([0.05286423, 0.36045732, 0.91513713, 0.02466782]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBmRfo7D5zG9",
        "colab_type": "text"
      },
      "source": [
        "This set of transformations raises a few questions. First, what was the point of it all? Why would we want to wrap objects this way instead of working with the raw Numpy arrays? The simple answer is for having a unified API for working with larger datasets. Suppose that `X` and `y` are so large that they can't fit easily into memory. What would we do then? Being able to work with an abstract `dataset` object proves very convenient then. In fact, you'll have reason to use this feature of `Dataset` later in the tutorial series.\n",
        "\n",
        "What else can we do with the `dataset` object? It turns out that it can be useful to be able to walk through the datapoints in the `dataset` one at a time. For that, we can use the `dataset.itersamples()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_8IONOw5zHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ad9dacca-d58d-44bf-d674-638547013e19"
      },
      "source": [
        "for x, y, _, _ in dataset.itersamples():\n",
        "    print(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.98945421 0.63065257 0.30835689 0.87841894] 0.05286423224531567\n",
            "[0.88537488 0.24523746 0.12397733 0.00886653] 0.36045732091017224\n",
            "[0.11237206 0.02017302 0.74253676 0.86894009] 0.9151371270770113\n",
            "[0.43141617 0.73671167 0.35075885 0.26500112] 0.024667824940694527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vU34w_e5zHH",
        "colab_type": "text"
      },
      "source": [
        "There are a couple of other fields that the `dataset` object tracks. The first is `dataset.ids`. This is a listing of unique identifiers for the datapoints in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fDXCKv_5zHI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba8983b7-730b-4af6-a655-a4dd98151c08"
      },
      "source": [
        "dataset.ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkbLR05r5zHQ",
        "colab_type": "text"
      },
      "source": [
        "In addition, the `dataset` object has a field `dataset.w`. This is the \"example weight\" associated with each datapoint. Since we haven't explicitly assigned the weights, this is simply going to be all ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uffH-1EI5zHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "637dec91-8691-4f75-a5d7-3c0ae7f393d9"
      },
      "source": [
        "dataset.w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHVs99Jh5zHU",
        "colab_type": "text"
      },
      "source": [
        "What if we want to set nontrivial weights for a dataset? One time we might want to do this is if we have a dataset where there are only a few positive examples to play with. It's pretty straightforward to do this with DeepChem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHiBOSJB5zHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d6f3f6f-2318-4bd0-9c2d-ba2a30dbd87f"
      },
      "source": [
        "w = np.random.random((4,)) # initializing weights with random vector of size 4x1\n",
        "dataset_with_weights = NumpyDataset(data, labels, w) # creates numpy dataset object\n",
        "dataset_with_weights.w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.29766842, 0.49396668, 0.37072533, 0.01817747])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLjtnas35zHk",
        "colab_type": "text"
      },
      "source": [
        "## MNIST Example\n",
        "\n",
        "Just to get a better understanding, we'll use the venerable MNIST dataset and use `NumpyDataset` to store the data. We're going to make use of the `tensorflow-datasets` package to facilitate our data reading. You'll need to install this package in order to make use of it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQSEHyoW5zHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install tensorflow-datasets\n",
        "## TODO(rbharath): Switch to stable version on release\n",
        "# TODO(rbharath): This only works on TF2. Uncomment once we've upgraded.\n",
        "#!pip install -q --upgrade tfds-nightly tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4qRvErO5zHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO(rbharath): This cell will only work with TF2 installed. Swap to this as default soon.\n",
        "\n",
        "#import tensorflow_datasets as tfds\n",
        "\n",
        "#data_dir = '/tmp/tfds'\n",
        "\n",
        "## Fetch full datasets for evaluation\n",
        "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
        "#mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
        "#mnist_data = tfds.as_numpy(mnist_data)\n",
        "#train_data, test_data = mnist_data['train'], mnist_data['test']\n",
        "#num_labels = info.features['label'].num_classes\n",
        "#h, w, c = info.features['image'].shape\n",
        "#num_pixels = h * w * c\n",
        "\n",
        "## Full train set\n",
        "#train_images, train_labels = train_data['image'], train_data['label']\n",
        "#train_images = np.reshape(train_images, (len(train_images), num_pixels))\n",
        "#train_labels = one_hot(train_labels, num_labels)\n",
        "\n",
        "## Full test set\n",
        "#test_images, test_labels = test_data['image'], test_data['label']\n",
        "#test_images = np.reshape(test_images, (len(test_images), num_pixels))\n",
        "#test_labels = one_hot(test_labels, num_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPTLNO6n5zH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# # Load the numpy data of MNIST into NumpyDataset\n",
        "# train = NumpyDataset(mnist.train.images, mnist.train.labels)\n",
        "# valid = NumpyDataset(mnist.validation.images, mnist.validation.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqOZyOsy5zH-",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at some of the data we've loaded so we can visualize our samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgAfsAdn5zH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Visualize one sample \n",
        "# sample = np.reshape(train.X[5], (28, 28))\n",
        "# plt.imshow(sample)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDfwAaNh5zIM",
        "colab_type": "text"
      },
      "source": [
        "## Converting a Numpy Array to tf.data.dataset()\n",
        "\n",
        "\n",
        "Let's say you want to use the `tf.data` module instead of DeepChem's data handling library. Doing this is straightforward and is quite similar to getting a `NumpyDataset` object from numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhbV376Z5zIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ff562f19-9261-48a5-9bc6-a759d9f9dc56"
      },
      "source": [
        "import tensorflow as tf\n",
        "data_small = np.random.random((4,5))\n",
        "label_small = np.random.random((4,))\n",
        "dataset = tf.data.Dataset.from_tensor_slices((data_small, label_small))\n",
        "print (\"Data\\n\")\n",
        "print (data_small)\n",
        "print (\"\\n Labels\")\n",
        "print (label_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data\n",
            "\n",
            "[[0.33040116 0.27228664 0.24498823 0.11302856 0.8087745 ]\n",
            " [0.40940497 0.01714215 0.00169625 0.54471045 0.08432139]\n",
            " [0.75675305 0.80432515 0.52047778 0.65493724 0.0941268 ]\n",
            " [0.8147976  0.15870959 0.791675   0.059836   0.72684409]]\n",
            "\n",
            " Labels\n",
            "[0.44813346 0.05086643 0.33214086 0.17735364]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPGKoCv05zIY",
        "colab_type": "text"
      },
      "source": [
        "## Extracting the numpy dataset from tf.data\n",
        "\n",
        "In order to extract the numpy array from the `tf.data`, you can just loop over the dataset created above like any other `for` loop in a `python` code. Let's have a look at how it's done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5L_u7YC5zIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "02b435c0-e912-458b-f824-58981589cf40"
      },
      "source": [
        "numpy_data = np.zeros((4,5))\n",
        "numpy_label = np.zeros((4,))\n",
        "\n",
        "counter = 0\n",
        "for data, label in dataset:\n",
        "  numpy_data[counter, :] = data\n",
        "  numpy_label[counter] = label\n",
        "  counter += 1\n",
        "        \n",
        "print(\"Numpy Data\")\n",
        "print(numpy_data)\n",
        "print(\"Numpy Label\")\n",
        "print(numpy_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numpy Data\n",
            "[[0.33040116 0.27228664 0.24498823 0.11302856 0.8087745 ]\n",
            " [0.40940497 0.01714215 0.00169625 0.54471045 0.08432139]\n",
            " [0.75675305 0.80432515 0.52047778 0.65493724 0.0941268 ]\n",
            " [0.8147976  0.15870959 0.791675   0.059836   0.72684409]]\n",
            "Numpy Label\n",
            "[0.44813346 0.05086643 0.33214086 0.17735364]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_IMrMth5zIh",
        "colab_type": "text"
      },
      "source": [
        "Now that you have the numpy arrays of `data` and `labels`, you can convert it to `NumpyDataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5DV_aLj5zIo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f1b256cc-1ac3-4176-f47c-2ced0fe5c0cb"
      },
      "source": [
        "dataset_ = NumpyDataset(numpy_data, numpy_label) # convert to NumpyDataset\n",
        "dataset_.X  # printing just to check if the data is same!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.33040116, 0.27228664, 0.24498823, 0.11302856, 0.8087745 ],\n",
              "       [0.40940497, 0.01714215, 0.00169625, 0.54471045, 0.08432139],\n",
              "       [0.75675305, 0.80432515, 0.52047778, 0.65493724, 0.0941268 ],\n",
              "       [0.8147976 , 0.15870959, 0.791675  , 0.059836  , 0.72684409]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltQfj-9n5zI_",
        "colab_type": "text"
      },
      "source": [
        "## Converting NumpyDataset to `tf.data`\n",
        "\n",
        "This can be easily done by the `itersamples()` method of `NumpyDataset`. This converts the `NumpyDataset` to `tf.data`. Let's look how it's done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVy39LEe5zJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a7dcd42b-e6f3-40f9-a896-100a38df7c1e"
      },
      "source": [
        "tf_dataset = tf.data.Dataset.from_tensor_slices((dataset_.X, dataset_.y))\n",
        "\n",
        "print (\"Tensorflow Data\")\n",
        "for data, label in tf_dataset:\n",
        "  print(data, label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow Data\n",
            "tf.Tensor([0.33040116 0.27228664 0.24498823 0.11302856 0.8087745 ], shape=(5,), dtype=float64) tf.Tensor(0.44813345846652675, shape=(), dtype=float64)\n",
            "tf.Tensor([0.40940497 0.01714215 0.00169625 0.54471045 0.08432139], shape=(5,), dtype=float64) tf.Tensor(0.05086643016201298, shape=(), dtype=float64)\n",
            "tf.Tensor([0.75675305 0.80432515 0.52047778 0.65493724 0.0941268 ], shape=(5,), dtype=float64) tf.Tensor(0.3321408648425913, shape=(), dtype=float64)\n",
            "tf.Tensor([0.8147976  0.15870959 0.791675   0.059836   0.72684409], shape=(5,), dtype=float64) tf.Tensor(0.17735364446502544, shape=(), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNoeh7sG5zJP",
        "colab_type": "text"
      },
      "source": [
        "# Using Splitters to split DeepChem Datasets\n",
        "\n",
        "In this section we will have a look at the various splitters that are present in deepchem library and how each of them can be used.\n",
        "\n",
        "### Index Splitter\n",
        "\n",
        "We start with the IndexSplitter. This splitter returns a range object which contains the split according to the fractions provided by the user. The three range objects can then be used to iterate over the dataset as test,valid and Train.\n",
        "\n",
        "Each of the splitters that will be used has two functions inherited from the main class that are `train_test_split` which can be used to split the data into training and tesing data and the other fucnction is `train_valid_test_split` which is used to split the data to train, validation and test split.\n",
        "\n",
        "Note: All the splitters have a default percentage of 80,10,10 as train, valid and test respectively. But can be changed by specifying the `frac_train`,`frac_test` and `frac_valid` in the ratio we want to split the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-MBPtBX5zJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "33a92a37-792a-42c0-84a6-6dc06c8101bb"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/deepchem/deepchem/master/deepchem/models/tests/example.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-05 14:08:00--  https://raw.githubusercontent.com/deepchem/deepchem/master/deepchem/models/tests/example.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 568 [text/plain]\n",
            "Saving to: ‘example.csv’\n",
            "\n",
            "\rexample.csv           0%[                    ]       0  --.-KB/s               \rexample.csv         100%[===================>]     568  --.-KB/s    in 0s      \n",
            "\n",
            "2020-08-05 14:08:01 (21.3 MB/s) - ‘example.csv’ saved [568/568]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs1xIWzo5zJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "current_dir=os.path.dirname(os.path.realpath('__file__'))\n",
        "input_data=os.path.join(current_dir,'example.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXHlTmdK5zJh",
        "colab_type": "text"
      },
      "source": [
        "We then featurize the data using any one of the featurizers present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN1lRtgC5zJi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8a3d6625-b4c8-43af-f1d7-a1e0652e81ce"
      },
      "source": [
        "import deepchem as dc\n",
        "\n",
        "tasks=['log-solubility']\n",
        "featurizer=dc.feat.CircularFingerprint(size=1024)\n",
        "loader = dc.data.CSVLoader(tasks=tasks, feature_field=\"smiles\",featurizer=featurizer)\n",
        "dataset=loader.create_dataset(input_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh7q0jGx5zJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from deepchem.splits.splitters import IndexSplitter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IemZbbvp5zJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splitter=IndexSplitter()\n",
        "train_data,valid_data,test_data=splitter.split(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6aE7YPn5zJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=[i for i in train_data]\n",
        "valid_data=[i for i in valid_data]\n",
        "test_data=[i for i in test_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkW5MLyL5zKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c059b667-6857-4a0e-97e6-e3b47cac6e36"
      },
      "source": [
        "len(train_data),len(valid_data),len(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7BQBpnP5zKG",
        "colab_type": "text"
      },
      "source": [
        "As we can see that without providing the user specifications on how to split the data, the data was split into a default of 80,10,10.\n",
        "\n",
        "But when we specify the parameters the dataset can be split according to our specificaitons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYeqhEgA5zKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "020e2365-405f-4052-9354-16d1e83d541d"
      },
      "source": [
        "train_data,valid_data,test_data=splitter.split(dataset,frac_train=0.7,frac_valid=0.2,frac_test=0.1)\n",
        "train_data=[i for i in train_data]\n",
        "valid_data=[i for i in valid_data]\n",
        "test_data=[i for i in test_data]\n",
        "len(train_data),len(valid_data),len(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVmV8dFe5zK1",
        "colab_type": "text"
      },
      "source": [
        "## Indice Splitter\n",
        "\n",
        "Another splitter present in the framework is `IndiceSplitter`. This splitter takes an input of valid_indices and test_indices which are lists with the indices of validation data and test data in the dataset respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCT3KKQz5zK2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a343d1-66d3-4df2-870a-7a97539a9737"
      },
      "source": [
        "from deepchem.splits.splitters import IndiceSplitter\n",
        "\n",
        "splitter=IndiceSplitter(valid_indices=[7],test_indices=[9])\n",
        "splitter.split(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1, 2, 3, 4, 5, 6, 8], [7], [9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROktroBH5zK6",
        "colab_type": "text"
      },
      "source": [
        "## RandomGroupSplitter\n",
        "\n",
        "The splitter which can be used to split the data on the basis of groupings is the `RandomGroupSplitter`. This splitter that splits on groupings. \n",
        "\n",
        "An example use case is when there are multiple conformations of the same molecule that share the same topology.This splitter subsequently guarantees that resulting splits preserve groupings.\n",
        "\n",
        "Note that it doesn't do any dynamic programming or something fancy to try to maximize the choice such that `frac_train`, `frac_valid`, or `frac_test` is maximized.It simply permutes the groups themselves. As such, use with caution if the number of elements per group varies significantly.\n",
        "\n",
        "The parameter that needs to be provided with the splitter is `groups`. This is an array like list of hashables which is the same as the size of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu_TRPslerPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4d1e557a-814d-434f-857b-14102f730e6a"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/deepchem/deepchem/master/deepchem/models/tests/example.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-05 14:08:06--  https://raw.githubusercontent.com/deepchem/deepchem/master/deepchem/models/tests/example.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 568 [text/plain]\n",
            "Saving to: ‘example.csv.1’\n",
            "\n",
            "\rexample.csv.1         0%[                    ]       0  --.-KB/s               \rexample.csv.1       100%[===================>]     568  --.-KB/s    in 0s      \n",
            "\n",
            "2020-08-05 14:08:06 (27.5 MB/s) - ‘example.csv.1’ saved [568/568]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jr7bNmneGMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is workaround...\n",
        "def load_solubility_data():\n",
        "  \"\"\"Loads solubility dataset\"\"\"\n",
        "  featurizer = dc.feat.CircularFingerprint(size=1024)\n",
        "  tasks = [\"log-solubility\"]\n",
        "  task_type = \"regression\"\n",
        "  loader = dc.data.CSVLoader(\n",
        "      tasks=tasks, smiles_field=\"smiles\", featurizer=featurizer)\n",
        "\n",
        "  return loader.featurize(\"example.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es-X6PDQ5zK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5b304195-0aff-4aa5-8313-6e6e81d9db72"
      },
      "source": [
        "from deepchem.splits.splitters import RandomGroupSplitter\n",
        "\n",
        "groups = [0, 4, 1, 2, 3, 7, 0, 3, 1, 0]\n",
        "solubility_dataset=load_solubility_data()\n",
        "\n",
        "splitter=RandomGroupSplitter(groups=groups)\n",
        "\n",
        "train_idxs, valid_idxs, test_idxs = splitter.split(solubility_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "smiles_field is deprecated and will be removed in a future version of DeepChem. Use feature_field instead.\n",
            "/usr/local/lib/python3.6/dist-packages/deepchem/data/data_loader.py:198: FutureWarning: featurize() is deprecated and has been renamed to create_dataset(). featurize() will be removed in DeepChem 3.0\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCYn9An75zLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "909dfc98-f783-4cae-e7c2-f8dc6119ee74"
      },
      "source": [
        "train_idxs,valid_idxs,test_idxs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([5, 0, 6, 9, 2, 8, 1], [4, 7], [3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW-jhqnr5zLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=[]\n",
        "for i in range(len(train_idxs)):\n",
        "    train_data.append(groups[train_idxs[i]])\n",
        "\n",
        "valid_data=[]\n",
        "for i in range(len(valid_idxs)):\n",
        "    valid_data.append(groups[valid_idxs[i]])\n",
        "\n",
        "test_data=[]\n",
        "for i in range(len(test_idxs)):\n",
        "    test_data.append(groups[test_idxs[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdiwca-U5zLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2042205f-6828-4244-9c6c-83b9f4f7bf0c"
      },
      "source": [
        "print(\"Groups present in the training data =\",train_data)\n",
        "print(\"Groups present in the validation data = \",valid_data)\n",
        "print(\"Groups present in the testing data = \", test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Groups present in the training data = [7, 0, 0, 0, 1, 1, 4]\n",
            "Groups present in the validation data =  [3, 3]\n",
            "Groups present in the testing data =  [2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3i_fjBt5zLs",
        "colab_type": "text"
      },
      "source": [
        "So the `RandomGroupSplitter` when properly assigned the groups, splits the data accordingly and preserves the groupings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He3vY6wu5zLu",
        "colab_type": "text"
      },
      "source": [
        "## Scaffold Splitter\n",
        "\n",
        "The `ScaffoldSplitter` splits the data based on the scaffold of small molecules. The splitter takes the data and generates scaffolds using the smiles in the data. Then the splitter sorts the data into scaffold sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Kkvi5F5zL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efc0b90c-7576-4aed-80d0-5d718e868c83"
      },
      "source": [
        "from deepchem.splits.splitters import ScaffoldSplitter\n",
        "\n",
        "splitter=ScaffoldSplitter()\n",
        "solubility_dataset=load_solubility_data()\n",
        "train_data,valid_data,test_data = splitter.split(solubility_dataset,frac_train=0.7,frac_valid=0.2,frac_test=0.1)\n",
        "len(train_data),len(valid_data),len(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "smiles_field is deprecated and will be removed in a future version of DeepChem. Use feature_field instead.\n",
            "/usr/local/lib/python3.6/dist-packages/deepchem/data/data_loader.py:198: FutureWarning: featurize() is deprecated and has been renamed to create_dataset(). featurize() will be removed in DeepChem 3.0\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhZxVoVs5zMa",
        "colab_type": "text"
      },
      "source": [
        "# Congratulations! Time to join the Community!\n",
        "\n",
        "Congratulations on completing this tutorial notebook! If you enjoyed working through the tutorial, and want to continue working with DeepChem, we encourage you to finish the rest of the tutorials in this series. You can also help the DeepChem community in the following ways:\n",
        "\n",
        "## Star DeepChem on [GitHub](https://github.com/deepchem/deepchem)\n",
        "This helps build awareness of the DeepChem project and the tools for open source drug discovery that we're trying to build.\n",
        "\n",
        "## Join the DeepChem Gitter\n",
        "The DeepChem [Gitter](https://gitter.im/deepchem/Lobby) hosts a number of scientists, developers, and enthusiasts interested in deep learning for the life sciences. Join the conversation!"
      ]
    }
  ]
}
